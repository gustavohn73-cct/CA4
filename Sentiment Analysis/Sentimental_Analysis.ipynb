{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d62e489-97ee-4bbb-b586-dd0ac317d74c",
   "metadata": {},
   "source": [
    "# Requeriments"
   ]
  },
  {
   "cell_type": "raw",
   "id": "433947a5-27f6-4ef4-bc42-56beef670ceb",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install textblob\n",
    "!pip install vaderSentiment\n",
    "!pip install nltk\n",
    "!pip install opendatasets\n",
    "!pip install textblob\n",
    "!pip install vaderSentiment\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c679a17-f94f-410d-8896-dd846235aa3f",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66eb792e-231f-4d79-9c5a-dc8cd1442db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "# Spark ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor\n",
    "\n",
    "\n",
    "\n",
    "#Spark SQL functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import from_utc_timestamp, udf, array_distinct, col, when\n",
    "from pyspark.sql.functions import regexp_replace, year, month, dayofmonth, hour, format_string\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Spark Datatypes\n",
    "from pyspark.sql.types import StringType, TimestampType, DateType, IntegerType\n",
    "from pyspark.sql.types import DoubleType, StructType, FloatType, StructField\n",
    "\n",
    "# Spark evaluation metrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "#Sentiment Analyzer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Hadoop\n",
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1369c9-74b3-48a7-9fb3-0bf35fc55299",
   "metadata": {},
   "source": [
    "## Warnings conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e22943-ba31-4eb5-b6e1-7f73dc104bf8",
   "metadata": {},
   "source": [
    "In this sections the warnings are suppressed, less logs while running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9a2026-e856-49e6-887e-3785b04ed00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing the warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc46987-087c-4635-aaa0-dbedb9d4be30",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bee1cb-6ae4-4ee7-8517-b7fe7845658c",
   "metadata": {},
   "source": [
    "## Reading Data from Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee164f-f374-49de-a701-753d1848ddd0",
   "metadata": {},
   "source": [
    "In this section we are reading data from Hadoop, using a Spark Session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed403b96-9f0f-46c1-922e-189b8ad1d25e",
   "metadata": {},
   "source": [
    "### Spark configurations and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda5d84f-37a4-4cc3-928e-395faa4c23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 19:34:11,624 WARN util.Utils: Your hostname, BDS-2023 resolves to a loopback address: 127.0.1.1; using 192.168.0.110 instead (on interface wlo1)\n",
      "2023-05-12 19:34:11,625 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-05-12 19:34:17,918 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def spark_start(restart = False, appname=\"HadoopAccess\"):\n",
    "\n",
    "    if restart == True:\n",
    "        spark.stop()\n",
    "    \n",
    "    # Configuration parameters for Spark\n",
    "    spark_conf = SparkConf().setMaster(\"local[*]\").setAppName(appname)\n",
    "\n",
    "    # Using SparkSession\n",
    "    spark = SparkSession.builder.config(conf=spark_conf).config('spark.sql.session.timeZone', 'UTC').getOrCreate()\n",
    "    \n",
    "     # this will help not to have too much error displaying\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel('ERROR')\n",
    "    \n",
    "    return spark\n",
    "\n",
    "\n",
    "spark = spark_start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1f8b4-d7f5-495e-b6c2-0758ef72ec01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tweets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0874e98-5c18-4908-9525-fe3e714963c4",
   "metadata": {},
   "source": [
    "This dataset was collect from Internet Archive, processed and stored on hadoop, following the Collect Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0899f708-5738-4eef-8d24-af43af5dfbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+--------------------+\n",
      "|         created_at|month|                text|            entities|\n",
      "+-------------------+-----+--------------------+--------------------+\n",
      "|2022-01-01 23:51:54|    1|RT @ampahcd: @Zac...|\"{\\\"hashtags\\\": [...|\n",
      "|2022-01-01 23:41:24|    1|RT @Rina_The_Espe...|\"{\\\"hashtags\\\": [...|\n",
      "|2022-01-01 23:41:28|    1|@VVitchStreams @R...|\"{\\\"hashtags\\\": [...|\n",
      "+-------------------+-----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Reading all parquets file in the folder tweets on Hadoop\n",
    "tweets = spark.read.parquet(\"/CA4/tweets/*.parquet\")\n",
    "\n",
    "#selecting just the columns wich we'll use in this analyse\n",
    "tweets = tweets.select(\"created_at\", \"month\", \"text\", \"entities\")\n",
    "\n",
    "#setting the column created_at as timestamp\n",
    "tweets = tweets.withColumn(\"created_at\", from_utc_timestamp(tweets[\"created_at\"], \"UTC\"))\n",
    "tweets.show(truncate=True, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d0e9b-ab6e-4c06-9a36-1d52973db94b",
   "metadata": {},
   "source": [
    "#### Exploring tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8880f017-c1eb-41d8-a5f3-7eebcbf9010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- entities: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab52d051-f9c6-49fc-b010-1637bbdf84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------------------+\n",
      "|summary|            month|                text|            entities|\n",
      "+-------+-----------------+--------------------+--------------------+\n",
      "|  count|           763266|              763266|              763266|\n",
      "|   mean|6.986469985562045|                null|                null|\n",
      "| stddev|4.225819505260059|                null|                null|\n",
      "|    min|                1|!\\nNew big data s...|\"{\\\"hashtags\\\": [...|\n",
      "|    25%|                2|                null|                null|\n",
      "|    50%|                7|                null|                null|\n",
      "|    75%|               11|                null|                null|\n",
      "|    max|               12|ðŸ«°ðŸ’°ðŸ’µ\\n\\nRT @OAN...|{\"urls\": [{\"url\":...|\n",
      "+-------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#as shown in the summary the dataframe has 763.266 lines\n",
    "tweets.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375665b7-3650-4bb8-a070-3ad82a5276b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+----------+\n",
      "|year|month| count|percentage|\n",
      "+----+-----+------+----------+\n",
      "|2021|   11|130286|    17.07%|\n",
      "|2021|   12|144352|    18.91%|\n",
      "|2022|    1|127260|    16.67%|\n",
      "|2022|    2| 67213|     8.81%|\n",
      "|2022|    3| 40585|     5.32%|\n",
      "|2022|    4| 31546|     4.13%|\n",
      "|2022|    5| 39132|     5.13%|\n",
      "|2022|    6| 38449|     5.04%|\n",
      "|2022|    7| 41471|     5.43%|\n",
      "|2022|    8| 32139|     4.21%|\n",
      "|2022|    9| 24553|     3.22%|\n",
      "|2022|   10| 46280|     6.06%|\n",
      "+----+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the distribution of tweets by month\n",
    "df = tweets.groupBy(year(\"created_at\").alias(\"year\"), month(\"created_at\").alias(\"month\")).count() \\\n",
    "                                 .orderBy([\"year\", \"month\"])\n",
    "#getting percentage\n",
    "df.withColumn(\"percentage\", format_string(\"%.2f%%\",((df[\"count\"]/tweets.count())*100))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5897b-276f-465a-9bd2-e5853bd79363",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165254b1-d4f9-491b-8249-4930c2574b34",
   "metadata": {},
   "source": [
    "Our model will be trained using a Dataset provided by user ardianumam on [Github](https://github.com/ardianumam/compilations/blob/master/ApacheSparkVideoSeries/dataset/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185c4e2d-0dbe-49df-85b1-cc5e659c0738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|that film is fant...|    1|\n",
      "|this music is rea...|    1|\n",
      "|winter is terribl...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file\n",
    "tweets_train = spark.read.csv('/CA4/tweets/training_database/tweets.csv', inferSchema=True, header=True)\n",
    "tweets_train = tweets_train.select(col(\"SentimentText\").alias(\"text\"), col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "tweets_train.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713ffb0-269c-43bc-b3d7-571760823580",
   "metadata": {},
   "source": [
    "#### Exploring training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b427dcf9-4584-4ac8-8b99-0dd7e9a67ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a7dbc6-d264-4ee6-9594-7eb8eda1eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+\n",
      "|summary|                text|              label|\n",
      "+-------+--------------------+-------------------+\n",
      "|  count|                1932|               1932|\n",
      "|   mean|                null|0.49585921325051757|\n",
      "| stddev|                null|  0.500112298992253|\n",
      "|    min|I adore cheese #b...|                  0|\n",
      "|    25%|                null|                  0|\n",
      "|    50%|                null|                  0|\n",
      "|    75%|                null|                  1|\n",
      "|    max|winter is the bes...|                  1|\n",
      "+-------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_train.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae5aaa33-f83e-4f6d-8461-89c690618749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "|label|count|percentage|\n",
      "+-----+-----+----------+\n",
      "|    1|  958|    49.59%|\n",
      "|    0|  974|    50.41%|\n",
      "+-----+-----+----------+\n",
      "\n",
      "Positive = 1\n",
      "Negative = 2\n"
     ]
    }
   ],
   "source": [
    "df = tweets_train.groupBy(\"label\").count()\n",
    "\n",
    "#getting percentage\n",
    "df.withColumn(\"percentage\", format_string(\"%.2f%%\",((df[\"count\"]/tweets_train.count())*100))).show()\n",
    "\n",
    "print(\"Positive = 1\")\n",
    "print(\"Negative = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c5a52-0fc8-4d6c-8f7c-7275e2d60588",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad50b5b-575f-4fee-b951-552387d7bf30",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6f37b-e791-4d8d-981a-bbf223d98e45",
   "metadata": {},
   "source": [
    "Following the steps of the [tutorial](https://github.com/ardianumam/compilations/blob/master/ApacheSparkVideoSeries/08%20Sentiment%20Analysis%20in%20Spark.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551352e6-6b72-41cf-b5cf-b01aac2ec3aa",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f2365-cbc3-4c36-b4d4-bc8fde18eeaf",
   "metadata": {},
   "source": [
    "#### Dividing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d96161-a538-456e-bc0f-bae973b9150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1516 \n",
      "Testing: 416\n"
     ]
    }
   ],
   "source": [
    "#80% training, 20% testing\n",
    "dividedData = tweets_train.randomSplit([0.8, 0.2]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "print (\"Training:\", trainingData.count(), \"\\nTesting:\", testingData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4be7e-9a25-4f6e-b714-7940049197e6",
   "metadata": {},
   "source": [
    "#### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0661bdd-6a67-40a0-b729-452ceadfa3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|     MeaningfulWords|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|[adore, cheese, #...|(262144,[1689,910...|\n",
      "|    1|[adore, cheese, #...|(262144,[1689,453...|\n",
      "|    1|[adore, cheese, #...|(262144,[1689,100...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "\n",
    "\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "\n",
    "\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")  \n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "\n",
    "numericTrainData.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85e11f-5d1d-486e-bdce-810fccb71091",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06065c70-aeaf-483d-91a5-5afb4ef651f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27299cc0-1a65-4679-a9f1-9ff957e318fc",
   "metadata": {},
   "source": [
    "#### Preparing Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9572310-b5d3-4f68-b221-266f79810f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|Label|MeaningfulWords                      |features                                               |\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|1    |[adore, cheese, #thumbs-up]          |(262144,[1689,88825,100089],[1.0,1.0,1.0])             |\n",
      "|1    |[adore, classical, music, #thumbs-up]|(262144,[88825,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14f1d3-3404-4d84-a6a7-5549002238d1",
   "metadata": {},
   "source": [
    "#### Predicting testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "437f54f8-2b67-4fae-ba67-a48c1de15e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----------+-----+\n",
      "|MeaningfulWords                      |prediction|Label|\n",
      "+-------------------------------------+----------+-----+\n",
      "|[adore, cheese, #thumbs-up]          |1.0       |1    |\n",
      "|[adore, classical, music, #thumbs-up]|1.0       |1    |\n",
      "|[adore, classical, music, #toptastic]|1.0       |1    |\n",
      "|[adore, skiing, #loveit]             |1.0       |1    |\n",
      "+-------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "Correct prediction: 409\n",
      "Total data: 416\n",
      "Accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "\n",
    "predictionFinal.show(n=4, truncate = False)\n",
    "\n",
    "correctPrediction = predictionFinal.filter(predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "\n",
    "totalData = predictionFinal.count()\n",
    "\n",
    "print(\"Correct prediction:\", correctPrediction) \n",
    "print(\"Total data:\", totalData)\n",
    "print(f\"Accuracy: {correctPrediction/totalData:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362e7d5-eab2-41c1-a14b-f89b89c17b77",
   "metadata": {},
   "source": [
    "### Tweets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1a5f0-f326-4a31-84bd-000f6f4c4b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7401633-931e-405e-ae2b-a5db08ef1b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+--------------------+--------------------+\n",
      "|         created_at|month|                text|            entities|        cleaned_text|\n",
      "+-------------------+-----+--------------------+--------------------+--------------------+\n",
      "|2022-01-01 23:51:54|    1|RT @ampahcd: @Zac...|\"{\\\"hashtags\\\": [...| We are blowing l...|\n",
      "|2022-01-01 23:41:24|    1|RT @Rina_The_Espe...|\"{\\\"hashtags\\\": [...| Vaccine aparthei...|\n",
      "|2022-01-01 23:41:28|    1|@VVitchStreams @R...|\"{\\\"hashtags\\\": [...| You have no prob...|\n",
      "|2022-01-01 23:52:58|    1|RT @drmeenalviz: ...|\"{\\\"hashtags\\\": [...| To round off 202...|\n",
      "|2022-01-01 23:53:11|    1|RT @JacobEdwardIn...|\"{\\\"hashtags\\\": [...| Im Covid positiv...|\n",
      "+-------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "at_regex = r\"@\\w+\" #Remove usernames\n",
    "link_regex = r\"http\\S+\" #Remove links\n",
    "rt_regex = r'\\bRT\\b' #Remove 'RT'\n",
    "ss_regex = r'[^\\w\\s]' #Remove Special strings\n",
    "ds_regex = r'\\s+' #remove spaces\n",
    "\n",
    "tweets = tweets.withColumn(\"cleaned_text\", regexp_replace(\"text\", at_regex, \"\").alias(\"text_without_at_signs\")) \\\n",
    "    .withColumn(\"cleaned_text\", regexp_replace(\"cleaned_text\", link_regex, \"\").alias(\"text_without_links\")) \\\n",
    "    .withColumn(\"cleaned_text\", regexp_replace(\"cleaned_text\", rt_regex, \"\").alias(\"text_without_regex\")) \\\n",
    "    .withColumn(\"cleaned_text\", regexp_replace(\"cleaned_text\", ss_regex, \"\").alias(\"text_without_regex\")) \\\n",
    "    .withColumn(\"cleaned_text\", regexp_replace(\"cleaned_text\", ds_regex, \" \").alias(\"text_without_regex\"))\n",
    "\n",
    "tweets.show(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3242ca5-099e-4349-9e08-3e2552656f83",
   "metadata": {},
   "source": [
    "#### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81fc0772-e551-43f0-9fb7-858d8994ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     MeaningfulWords|            features|\n",
      "+--------------------+--------------------+\n",
      "|[, blowing, large...|(262144,[3928,510...|\n",
      "|[, vaccine, apart...|(262144,[32890,57...|\n",
      "|[, problem, injec...|(262144,[31536,76...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "tokenizedData = tokenizer.transform(tweets)\n",
    "\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemoved = swr.transform(tokenizedData)\n",
    "\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericData = hashTF.transform(SwRemoved).select('MeaningfulWords', 'features')\n",
    "\n",
    "\n",
    "numericData.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a8525-027c-4f45-a771-46fb07f4516b",
   "metadata": {},
   "source": [
    "#### Predicting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a593bab-e3ab-4078-b59e-8a9967db210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(numericData)\n",
    "\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ea7eb7a-f85c-4a87-b94a-32eaaf3eb7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+----------+\n",
      "|MeaningfulWords                                                                                             |prediction|\n",
      "+------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[, blowing, large, holes, entire, pandemic, vaccination, agendabig, pharma]                                 |0.0       |\n",
      "|[, vaccine, apartheid, actually, exists, even, imperial, core, keep, mind]                                  |0.0       |\n",
      "|[, problem, injected, completely, unproven, vaccine, still, go]                                             |0.0       |\n",
      "|[, round, 2021, mum, bumped, old, friend, street, told, us, wouldnt, take, vaccine, amp, im]                |0.0       |\n",
      "|[, im, covid, positive, receiving, moderna, vaccines, im, still, able, show, work, er, based, cdc]          |0.0       |\n",
      "|[, youre, protesting, pfizers, covid, vaccine, still, popping, viagra, made, pfizer, makes, hypocrite, happ]|0.0       |\n",
      "|[, three, debacles, cards, far, high, incidence, severe, side, effects, possibility, vaccines, ge]          |0.0       |\n",
      "|[, religion, opposes, vaccines]                                                                             |0.0       |\n",
      "|[, choice, weigh, risks, vaccine, worked, great, concern]                                                   |1.0       |\n",
      "|[, la, county, incredibly, strict, vaccine, passports, worlds, longest, mask, mandates, stop]               |0.0       |\n",
      "+------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionFinal.show(truncate = False, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b535e4-ee0c-455b-99cc-75bf956ce132",
   "metadata": {},
   "source": [
    "#### Join Prediction with Tweets Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fae7281f-d964-4fcb-816d-9fe9853c5299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763266"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionFinal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd110439-62bb-4451-8ff5-55dffe9c2ec2",
   "metadata": {},
   "source": [
    "As the forecast dataframe is following the tweet dataframe, it is possible to combine the two through an autoincremented index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb2a8319-3f9a-4828-b0dc-2cbe7864ce0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+-----+----------+\n",
      "|         created_at|                text|        cleaned_text|            entities|month|prediction|\n",
      "+-------------------+--------------------+--------------------+--------------------+-----+----------+\n",
      "|2022-01-01 23:51:54|RT @ampahcd: @Zac...| We are blowing l...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:37:00|@ruiz20059 No.  W...| No What religion...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:59:51|RT @WSJ: Internat...| International tr...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:45:39|RT @toadmeister: ...| A major study fr...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:46:11|@doctor_oxford Nu...| Nurses are too b...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:46:38|RT @Madisontx76: ...| Why do Democrats...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:29:36|RT @JasonLehn: If...| If you think hav...|\"{\\\"hashtags\\\": [...|    1|       1.0|\n",
      "|2022-01-01 23:12:57|RT @cooperlund: I...| It may seem like...|\"{\\\"hashtags\\\": [...|    1|       1.0|\n",
      "|2022-01-01 23:31:00|@7NewsSydney She ...| She doesnt seem ...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:49:33|RT @gedkearney: N...| New year Same pr...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 22:56:00|RT @SquireforBran...| My reserve of Si...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:42:59|RT @birgitomo: Mu...| Must read Thread...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:39:13|@kceelake âœï¸Ameri...| American Heart J...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:38:17|RT @JacobEdwardIn...| Im Covid positiv...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:38:33|@tammysingley13 @...| I think he means...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 22:42:55|RT @FurlongMick: ...| When it comes to...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:33:32|RT @txsalth2o: Si...| Sincere question...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 22:31:42|RT @Undergroundco...| Those medical mi...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 22:36:56|RT @JesseKellyDC:...| 5 In one of the ...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "|2022-01-01 23:25:48|RT @JesseKellyDC:...| 5 In one of the ...|\"{\\\"hashtags\\\": [...|    1|       0.0|\n",
      "+-------------------+--------------------+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a column with id following the data's order \n",
    "tweets = tweets.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "predictionFinal = predictionFinal.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# join by \"row_id\"\n",
    "tweets_pred = tweets.select('row_id','created_at', 'text', 'cleaned_text', 'entities','month') \\\n",
    "                    .join(predictionFinal.select('row_id', 'prediction'), \"row_id\", \"inner\")\n",
    "                \n",
    "\n",
    "# drop column \n",
    "tweets_pred = tweets_pred.drop(\"row_id\")\n",
    "\n",
    "tweets_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a9c68-a63e-4211-867a-3b9b30dfe9f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Textblob and Varder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42d51630-94bf-4e1d-8863-ecde8fb3ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def f_textblob(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def f_vader(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)['compound']\n",
    "\n",
    "\n",
    "#UDFs\n",
    "udf_textblob = udf(f_textblob, StringType())\n",
    "\n",
    "udf_vader = udf(f_vader, StringType())\n",
    "\n",
    "\n",
    "#applying to Dataframe\n",
    "tweets_pred = tweets_pred.withColumn(\"textblob\", udf_textblob(tweets_pred[\"cleaned_text\"])) \\\n",
    "                         .withColumn(\"vader\", udf_vader(tweets_pred[\"cleaned_text\"]))\n",
    "\n",
    "#tweets_pred_2.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44a3a3-e727-4e0f-9432-6961c8353713",
   "metadata": {},
   "source": [
    "## Making our Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf9a59-7da4-4a7b-ab88-8b6dd286f1da",
   "metadata": {},
   "source": [
    "The Score was calculated taking into account that Varder and TextBlob are tools developed for this type of analysis, they were given a weight of 1.5, the other analyzes had a weight of 1 out of a total of 5. In this way, it will be indicated as positive (1) if it is greater than zero otherwise it will be negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5998249c-504b-44e4-a288-da9120d73ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pred = tweets_pred.withColumn(\"score\", ((col(\"prediction\") + (col(\"textblob\")*1.5) + (col(\"vader\")*1.5)) / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a82e79b-0780-4bd3-b5b5-631451878535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-------+--------------------+\n",
      "|        cleaned_text|prediction|            textblob|  vader|               score|\n",
      "+--------------------+----------+--------------------+-------+--------------------+\n",
      "| We are blowing l...|       0.0| 0.10714285714285714|    0.0|0.040178571428571425|\n",
      "| No What religion...|       0.0|                 0.0| -0.296|-0.11099999999999999|\n",
      "| International tr...|       0.0| 0.04545454545454545|    0.0|0.017045454545454544|\n",
      "| A major study fr...|       0.0|             0.03125|-0.4939|         -0.17349375|\n",
      "| Nurses are too b...|       0.0|                -0.2|    0.0|-0.07500000000000001|\n",
      "| Why do Democrats...|       0.0|                 0.2|-0.6115|-0.15431250000000002|\n",
      "| If you think hav...|       1.0|                 0.0| 0.3612|             0.38545|\n",
      "| It may seem like...|       1.0|-0.16666666666666666| 0.1127| 0.22976249999999998|\n",
      "| She doesnt seem ...|       0.0|  0.4666666666666667|    0.0|               0.175|\n",
      "| New year Same pr...|       0.0| 0.03409090909090909|-0.6369| -0.2260534090909091|\n",
      "+--------------------+----------+--------------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_pred.select(\"cleaned_text\", \"prediction\", \"textblob\", \"vader\", \"score\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c89471f-7688-466d-8a7f-125e4a0b9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- entities: string (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- textblob: string (nullable = true)\n",
      " |-- vader: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b2f56-a847-4bbc-93e8-d424f5f7a4f4",
   "metadata": {},
   "source": [
    "### Saving on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a02dd106-53ee-45d5-9d19-b4654cb6fbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to check if the file is already in Hadoop, if not this will put there.\n",
    "\n",
    "def spark2hadoop(df, folder, partitionBy = False, spark = spark):\n",
    "\n",
    "    client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "    if folder in client.list('/CA4/'):\n",
    "        print('The files are already in Hadoop, reading files.')\n",
    "        df = spark.read.parquet(f\"/CA4/{folder}/**/*.parquet\")\n",
    "\n",
    "    else:\n",
    "        print('Putting on Hadoop.')\n",
    "        if not partitionBy == False:\n",
    "            df.write.partitionBy(partitionBy).parquet(f\"/CA4/{folder}\")\n",
    "            print(f\"Save at /CA4/{folder} partitioned by {partitionBy}\")\n",
    "        else:\n",
    "            df.write.parquet(f\"/CA4/{folder}\")\n",
    "            print(f\"Save at /CA4/{folder}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeb17fa3-921b-4357-9573-50fabd43f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files are already in Hadoop, reading files.\n"
     ]
    }
   ],
   "source": [
    "#Checking dataframe\n",
    "tweets_pred_saved = spark2hadoop(df = tweets_pred, folder = \"sentiment\", partitionBy = \"month\", spark = spark)\n",
    "\n",
    "#tweets_pred.write.partitionBy(\"month\").parquet(\"/CA4/predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e127318-810b-4ab4-a89a-3a8233fd79d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+----------+--------+-------+----------+\n",
      "|         created_at|                text|        cleaned_text|            entities|prediction|textblob|  vader|     score|\n",
      "+-------------------+--------------------+--------------------+--------------------+----------+--------+-------+----------+\n",
      "|2021-12-01 23:55:38|RT @9thfloor: MAR...| MARCH OF THE VAC...|\"{\\\"hashtags\\\": [...|       0.0|    -0.2|-0.9087|-0.4157625|\n",
      "|2021-12-01 23:56:36|@GLHolcombe @ucjo...| If you cant see ...|\"{\\\"hashtags\\\": [...|       0.0|     0.0|    0.0|       0.0|\n",
      "+-------------------+--------------------+--------------------+--------------------+----------+--------+-------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_pred_saved.show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfcb2a1-c76f-4c24-957e-9369c40ed719",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29dd464e-a654-4118-8cfb-743f936d12c8",
   "metadata": {},
   "source": [
    "!pip install internetarchive\n",
    "!pip install mysql-connector-python\n",
    "!pip install libtorrent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d93c2-4b48-4cf7-a445-89db0a09a599",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b36d8b9-4b17-4bea-bbfd-a4beb7244d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import internetarchive\n",
    "import os\n",
    "import tarfile\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import mysql.connector\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e7c5cad-7e70-46e9-9484-0209505e912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing the warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef408647-f9e9-448b-85ef-9f1772efb9c8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c9a17-78f2-4d93-8404-cddb59dd43c9",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4887335f-3783-4a95-94c1-79c422c428f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(name_folder):            \n",
    "    #Creating folder if that doesn't exist\n",
    "    p = %pwd\n",
    "    p = p + f'/{name_folder}'\n",
    "    path = os.path.expanduser(p)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(\"{} created.\".format(path))\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "186417d1-fca8-4dcc-907e-da5c3591a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(item_name, ext='*tar'):\n",
    "    \n",
    "    file_names = [f.name for f in internetarchive.get_files(item_name, glob_pattern= ext)]\n",
    "    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8224599-ab0a-43db-a4af-63ceaaaf5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize(size_bytes):\n",
    "    KB = 1 << 10\n",
    "    MB = 1 << 20\n",
    "    GB = 1 << 30\n",
    "\n",
    "    if size_bytes < KB:\n",
    "        return '{} B'.format(size_bytes)\n",
    "    elif size_bytes < MB:\n",
    "        return '{:.2f} KiB'.format(size_bytes/KB)\n",
    "    elif size_bytes < GB:\n",
    "        return '{:.2f} MiB'.format(size_bytes/MB)\n",
    "    else:\n",
    "        return '{:.2f} GiB'.format(size_bytes/GB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a526cb-8bb0-4bdc-a383-b0e5841cab27",
   "metadata": {},
   "source": [
    "## Cleaning and Transfomations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12e37500-3043-48aa-8403-3ac8f136f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path, file_name, word_bag = False, lang = False, db_update = False):\n",
    "    \n",
    "    #reading json\n",
    "    df = pd.read_json(file_path, lines=True, compression='gzip')\n",
    "    \n",
    "    #updating total tweets\n",
    "    if not update_db == False:\n",
    "        update_db(table = 'control', field = f\"count_total = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
    "    \n",
    "    #Filtering language if a language was sent\n",
    "    if not lang == False:\n",
    "        df = df[df.lang == lang]\n",
    "    \n",
    "    #taking away columns unnecessary\n",
    "    df = df.loc[:, ['created_at', 'text', 'entities']]\n",
    "    \n",
    "    #filtering Tweets using subject word bag if a word bag was sent\n",
    "    if not word_bag == False:\n",
    "        bow = '|'.join(word_bag) # bag of word\n",
    "        df = df[df['text'].str.contains(bow, case=False)]\n",
    "        \n",
    "        #updating filtered tweets\n",
    "        if not update_db == False:\n",
    "            update_db(table = 'control', field = f\"count_filtered = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267d74a-1829-44cf-a53e-f0597320bf1d",
   "metadata": {},
   "source": [
    "## Control Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af3344-6b81-4c18-a504-70041f78d9be",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc07a036-7be4-41c0-92d4-c4f489a34a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_op_csv(control_file):\n",
    "\n",
    "    #if file doesn't exist I'll create it\n",
    "    if not os.path.exists(control_file):\n",
    "        with open(control_file, 'w', newline='') as control_csv:\n",
    "            writer = csv.DictWriter(control_csv, fieldnames=['name','datetime'])\n",
    "            writer.writeheader()\n",
    "        control_csv.close()\n",
    "        print(\"{} created.\".format(control_file))\n",
    "            \n",
    "    #Reading the control file\n",
    "    with open(control_file, 'r', newline='') as control_csv:\n",
    "        reader = csv.DictReader(control_csv)\n",
    "        reader_data = [r for r in reader]\n",
    "        control_csv.seek(0)\n",
    "    control_csv.close()\n",
    "    \n",
    "    return reader_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "857497e9-a0cf-4c51-ac3b-32b583edad21",
   "metadata": {},
   "source": [
    "path = check_folder('Downloads')\n",
    "\n",
    "#calling function for getting control file\n",
    "control_path = path + '/control_download.csv'    \n",
    "reader_data = control_op_csv(control_path)\n",
    "reader_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0502fcf3-2628-4ac4-b0d8-6145dac56060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_csv(item_name, file_name, download_folder = 'Downloads'):\n",
    "    \n",
    "    path = check_folder(download_folder)\n",
    "    \n",
    "    #calling function for getting control file\n",
    "    control_path = path + '/control_download.csv'    \n",
    "    reader_data = control_op_csv(control_path)\n",
    "    \n",
    "    #Getting Itens names\n",
    "    item = internetarchive.get_item(item_name)\n",
    "        \n",
    "    #openning control file to add file names on the list\n",
    "    with open(control_path, 'a', newline='') as control_download:\n",
    "        writer = csv.DictWriter(control_download, fieldnames=['name','datetime'])\n",
    "        \n",
    "        #check if the file has been downloaded before\n",
    "        if not any(row['name'] == file_name for row in reader_data):\n",
    "            \n",
    "            #downloading\n",
    "            r = item.download(\n",
    "                destdir=path,  # The directory to download files to\n",
    "                ignore_existing=True,  # Skip files that already exist locally\n",
    "                checksum=True,  # Skip files based on checksum\n",
    "                verbose=True,  # Print progress to stdout\n",
    "                retries=100,  # The number of times to retry on failed requests\n",
    "                no_directory=True,  # Download withtout the identifier\n",
    "                files = file_name)\n",
    "            \n",
    "            #Adding file name on control list\n",
    "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    control_download.close()\n",
    "    \n",
    "    path_file = path + f'/{file_name}'\n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ee9268d-0988-4248-9c72-bb22753af55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_file_csv(tar_file, file_name, extration_folder='Extraction'):\n",
    "\n",
    "    path = check_folder(extration_folder)\n",
    "    \n",
    "    #calling function for getting control file\n",
    "    control_path = path + '/control_extraction.csv'    \n",
    "    reader_data = control_op(control_path)\n",
    "\n",
    "    #openning control file to add file names on the list\n",
    "    with open(control_path, 'a', newline='') as control_tar:\n",
    "        writer = csv.DictWriter(control_tar, fieldnames=['name','datetime'])\n",
    "        \n",
    "        #check if the file has been extrated before\n",
    "        if not any(row['name'] == file_name for row in reader_data):\n",
    "\n",
    "            #Extracting file\n",
    "            tar_file.extract(file_name, path=path)\n",
    "\n",
    "            #Saving files name on control file\n",
    "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            path_file = path + f'/{file_name}'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            path_file = False\n",
    "    \n",
    "    control_tar.close()\n",
    "    \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530faf1b-3818-4358-90d1-2dc89ee170c8",
   "metadata": {},
   "source": [
    "### Mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6bc96-91f0-481b-b4bf-ab17cb3e0ba6",
   "metadata": {},
   "source": [
    "#### Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51b47408-deae-439f-84ee-ce0e09139d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_connection():\n",
    "\n",
    "    #getting configuration\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "\n",
    "    # Connect to Mysql\n",
    "    conn = mysql.connector.connect(\n",
    "        host = config['mysql']['host'],\n",
    "        user = config['mysql']['user'],\n",
    "        password = config['mysql']['password'],\n",
    "        database = config['mysql']['database']\n",
    "    )\n",
    "        \n",
    "    return conn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce187950-ba61-43da-b3b2-adcc26f74af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(table, field, condition):\n",
    "    \n",
    "    conn = open_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"UPDATE {table} SET {field} WHERE {condition};\")\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74186bbb-ce88-48c8-aaf4-6dd0fbda7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query):\n",
    "    \n",
    "    conn = open_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(query)\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2164244a-d491-42c2-bd19-2f62263ce64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db(query):\n",
    "    conn = open_connection()\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340d245-7251-4b01-879e-187a50119290",
   "metadata": {},
   "source": [
    "#### Specific operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d10c8ed-7d8a-4eae-84b3-2c11f7e62584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_control_db(file_type, file_name = False):\n",
    "    \n",
    "    if file_name == False:\n",
    "        \n",
    "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}'\").to_dict(orient='records')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}' AND name = '{file_name}'\").to_dict(orient='records')\n",
    "        \n",
    "        if len(check) > 0:\n",
    "            check = True\n",
    "        else:\n",
    "            check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "808f97b8-16b5-45c4-aa8e-5c2f16c7e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_db(item_name, file_name, download_folder = 'Downloads', type_control = 'download'):\n",
    "    \n",
    "    path = check_folder(download_folder)\n",
    "    \n",
    "    #query to get information from Mysql about control\n",
    "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
    "                             \n",
    "    #check if the file has been unzipped before\n",
    "    if check == False:\n",
    "    \n",
    "        #getting Itens names\n",
    "        item = internetarchive.get_item(item_name)\n",
    "\n",
    "        #downloading\n",
    "        r = item.download(\n",
    "            destdir=path,  # The directory to download files to\n",
    "            ignore_existing=True,  # Skip files that already exist locally\n",
    "            checksum=True,  # Skip files based on checksum\n",
    "            verbose=False,  # Print progress to stdout\n",
    "            retries=100,  # The number of times to retry on failed requests\n",
    "            no_directory=True,  # Download withtout the identifier\n",
    "            files = file_name)\n",
    "        \n",
    "        #getting Metadata\n",
    "        metadata = list(filter(lambda p: p[\"name\"] == file_name, item.item_metadata['files']))\n",
    "\n",
    "        #Adding file name on control list\n",
    "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        sz = metadata[0]['size']\n",
    "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}', '{dt}', '{type_control}', '{sz}')\")\n",
    "    \n",
    "        path_file = path + f'/{file_name}'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        path_file = False   \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff8a6aea-def3-47d2-b9dc-c85e10ba56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_file_db(tar_file, file_name, extration_folder='Extraction', type_control = 'extraction'):\n",
    "    \n",
    "    path = check_folder(extration_folder)\n",
    "    \n",
    "    #query to get information from Mysql about control\n",
    "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
    "    \n",
    "    \n",
    "    #check if the file has been unzipped before\n",
    "    if check == False:\n",
    "\n",
    "        #Extracting file\n",
    "        tar_file.extract(file_name, path=path)\n",
    "\n",
    "        #Adding file name on control list\n",
    "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        sz = tar_file.getmember(name=file_name).size\n",
    "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}', '{dt}', '{type_control}', '{sz}')\")\n",
    "        \n",
    "        path_file = path + f'/{file_name}'\n",
    "    \n",
    "    else: #case the file has been unzipped\n",
    "            \n",
    "        path_file = False\n",
    "        \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fc0a960-497e-4a17-80a6-a03d5dbebacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idnew_table</th>\n",
       "      <th>name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>count_total</th>\n",
       "      <th>count_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16509</td>\n",
       "      <td>twitter-stream-20220801.tar</td>\n",
       "      <td>2023-05-01 00:06:50</td>\n",
       "      <td>download</td>\n",
       "      <td>2742671360</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23434</td>\n",
       "      <td>twitter-stream-20220802.tar</td>\n",
       "      <td>2023-05-01 01:34:20</td>\n",
       "      <td>download</td>\n",
       "      <td>2483517440</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30440</td>\n",
       "      <td>twitter-stream-20220803.tar</td>\n",
       "      <td>2023-05-01 02:51:00</td>\n",
       "      <td>download</td>\n",
       "      <td>2592880640</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33554</td>\n",
       "      <td>twitter-stream-20220804.tar</td>\n",
       "      <td>2023-05-01 04:05:47</td>\n",
       "      <td>download</td>\n",
       "      <td>2329610240</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39144</td>\n",
       "      <td>twitter-stream-20220805.tar</td>\n",
       "      <td>2023-05-01 05:23:17</td>\n",
       "      <td>download</td>\n",
       "      <td>2664581120</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41935</td>\n",
       "      <td>twitter-stream-20220806.tar</td>\n",
       "      <td>2023-05-01 06:39:27</td>\n",
       "      <td>download</td>\n",
       "      <td>2590279680</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45686</td>\n",
       "      <td>twitter-stream-20220807.tar</td>\n",
       "      <td>2023-05-01 07:55:21</td>\n",
       "      <td>download</td>\n",
       "      <td>2614466560</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50671</td>\n",
       "      <td>twitter-stream-20220808.tar</td>\n",
       "      <td>2023-05-01 09:22:42</td>\n",
       "      <td>download</td>\n",
       "      <td>2580101120</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54948</td>\n",
       "      <td>twitter-stream-20220809.tar</td>\n",
       "      <td>2023-05-01 12:14:34</td>\n",
       "      <td>download</td>\n",
       "      <td>2678353920</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62485</td>\n",
       "      <td>twitter-stream-20220810.tar</td>\n",
       "      <td>2023-05-01 13:44:05</td>\n",
       "      <td>download</td>\n",
       "      <td>2665984000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>67809</td>\n",
       "      <td>twitter-stream-20220811.tar</td>\n",
       "      <td>2023-05-01 15:24:23</td>\n",
       "      <td>download</td>\n",
       "      <td>2581882880</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72795</td>\n",
       "      <td>twitter-stream-20220812.tar</td>\n",
       "      <td>2023-05-01 17:54:36</td>\n",
       "      <td>download</td>\n",
       "      <td>2726369280</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idnew_table                         name            datetime      type  \\\n",
       "0         16509  twitter-stream-20220801.tar 2023-05-01 00:06:50  download   \n",
       "1         23434  twitter-stream-20220802.tar 2023-05-01 01:34:20  download   \n",
       "2         30440  twitter-stream-20220803.tar 2023-05-01 02:51:00  download   \n",
       "3         33554  twitter-stream-20220804.tar 2023-05-01 04:05:47  download   \n",
       "4         39144  twitter-stream-20220805.tar 2023-05-01 05:23:17  download   \n",
       "5         41935  twitter-stream-20220806.tar 2023-05-01 06:39:27  download   \n",
       "6         45686  twitter-stream-20220807.tar 2023-05-01 07:55:21  download   \n",
       "7         50671  twitter-stream-20220808.tar 2023-05-01 09:22:42  download   \n",
       "8         54948  twitter-stream-20220809.tar 2023-05-01 12:14:34  download   \n",
       "9         62485  twitter-stream-20220810.tar 2023-05-01 13:44:05  download   \n",
       "10        67809  twitter-stream-20220811.tar 2023-05-01 15:24:23  download   \n",
       "11        72795  twitter-stream-20220812.tar 2023-05-01 17:54:36  download   \n",
       "\n",
       "          size count_total count_filtered  \n",
       "0   2742671360        None           None  \n",
       "1   2483517440        None           None  \n",
       "2   2592880640        None           None  \n",
       "3   2329610240        None           None  \n",
       "4   2664581120        None           None  \n",
       "5   2590279680        None           None  \n",
       "6   2614466560        None           None  \n",
       "7   2580101120        None           None  \n",
       "8   2678353920        None           None  \n",
       "9   2665984000        None           None  \n",
       "10  2581882880        None           None  \n",
       "11  2726369280        None           None  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_db(\"SELECT * FROM control WHERE type = 'download'AND name LIKE 'twitter-stream-202208%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95611249-11d4-4eab-88ba-9dfb6f24ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1440 entries, 0 to 1439\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   idnew_table     1440 non-null   int64         \n",
      " 1   name            1440 non-null   object        \n",
      " 2   datetime        1440 non-null   datetime64[ns]\n",
      " 3   type            1440 non-null   object        \n",
      " 4   size            1440 non-null   object        \n",
      " 5   count_total     1440 non-null   object        \n",
      " 6   count_filtered  1440 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(5)\n",
      "memory usage: 78.9+ KB\n"
     ]
    }
   ],
   "source": [
    "read_db(\"SELECT * FROM control WHERE type = 'extraction' AND name LIKE '20221022%'\").info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebdffcd5-3416-4eed-8618-3950aeabef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_control_db(file_type = 'download'))#, file_name = '20221001/20221001235900.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee5629-db75-4b84-9840-753eec3876dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45e74d56-fa23-4898-a516-d426a4faf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_db(\"SELECT * FROM control WHERE type = 'extraction'\")\n",
    "down_df = check_control_db(file_type = 'download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eec1e704-d36d-437a-9f73-facb9c4a1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'] = df['name'].str[:8].apply(lambda x: datetime.strptime(x, '%Y%m%d').strftime('%Y-%m-%d'))\n",
    "df['data'] = pd.to_datetime(df['data'])\n",
    "df['size'] = df['size'].astype(int)\n",
    "df['count_total'] = df['count_total'].astype(int)\n",
    "df['count_filtered'] = df['count_filtered'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "565029d7-5242-4f98-a47d-d723a1949e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of days: 57 \n",
      "\n",
      "Total Downloaded: 140.50 GiB\n",
      "An average of 2.46 GiB per day downloaded \n",
      "\n",
      "Total of Tweets: 214,899,850\n",
      "An average of 3,770,173 tweets downloaded per day \n",
      "\n",
      "Tweets filtered: 65,762\n",
      "An average of 1,154 tweets filtered per day \n",
      "\n"
     ]
    }
   ],
   "source": [
    "donwloaded = df.groupby(pd.Grouper(key='data', freq='M'))['size'].sum()\n",
    "print(f\"Total of days: {len(down_df)} \\n\")\n",
    "\n",
    "print(f\"Total Downloaded: {humanize(donwloaded.sum())}\")\n",
    "print(f\"An average of {humanize(donwloaded.sum()/len(down_df))} per day downloaded \\n\")\n",
    "\n",
    "print(\"Total of Tweets: {:,}\".format(df['count_total'].sum()))\n",
    "print(f\"An average of {df['count_total'].sum()/len(down_df):,.0f} tweets downloaded per day \\n\")\n",
    "\n",
    "print(\"Tweets filtered: {:,}\".format(df['count_filtered'].sum()))\n",
    "print(f\"An average of {df['count_filtered'].sum()/len(down_df):,.0f} tweets filtered per day \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2966b9-9f12-4b1a-bc3b-4cd40d6669ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

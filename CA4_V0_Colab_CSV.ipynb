{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustavohn73/CA4/blob/main/CA4_V0_Colab_CSV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfcb2a1-c76f-4c24-957e-9369c40ed719",
      "metadata": {
        "id": "fbfcb2a1-c76f-4c24-957e-9369c40ed719"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install internetarchive"
      ],
      "metadata": {
        "id": "UpNHuQP1kaY1",
        "outputId": "b7fcc74d-d407-4a3e-b01b-29dff054349b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UpNHuQP1kaY1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting internetarchive\n",
            "  Downloading internetarchive-3.4.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.1/101.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt<0.7.0,>=0.6.0\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonpatch>=0.4\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from internetarchive) (2.27.1)\n",
            "Collecting schema>=0.4.0\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from internetarchive) (4.65.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from internetarchive) (1.26.15)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->internetarchive) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->internetarchive) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->internetarchive) (2.0.12)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema>=0.4.0->internetarchive) (0.6.0.post1)\n",
            "Building wheels for collected packages: internetarchive, docopt\n",
            "  Building wheel for internetarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for internetarchive: filename=internetarchive-3.4.0-py2.py3-none-any.whl size=95526 sha256=227e133a5d93b30cc0ac2ae232986eb2a0e4fff900fd91e37f073bd92bef4aca\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/b4/5a/9de9be35f2d1988ab17231fe1f8bcd258d57ddbac7eb143b73\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=f98312a09490c8d9c57fe5a67600ab401c6258b6397922a43124b44c457f8c65\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built internetarchive docopt\n",
            "Installing collected packages: docopt, schema, jsonpointer, jsonpatch, internetarchive\n",
            "Successfully installed docopt-0.6.2 internetarchive-3.4.0 jsonpatch-1.32 jsonpointer-2.3 schema-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "raw",
      "id": "29dd464e-a654-4118-8cfb-743f936d12c8",
      "metadata": {
        "id": "29dd464e-a654-4118-8cfb-743f936d12c8"
      },
      "source": [
        "!pip install internetarchive\n",
        "!pip install mysql-connector-python\n",
        "!pip install libtorrent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492d93c2-4b48-4cf7-a445-89db0a09a599",
      "metadata": {
        "id": "492d93c2-4b48-4cf7-a445-89db0a09a599"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vi8-IT6gkold",
        "outputId": "c97c059c-c796-446a-e81e-200d1080462d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Vi8-IT6gkold",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b36d8b9-4b17-4bea-bbfd-a4beb7244d67",
      "metadata": {
        "id": "7b36d8b9-4b17-4bea-bbfd-a4beb7244d67"
      },
      "outputs": [],
      "source": [
        "import internetarchive\n",
        "import os\n",
        "import tarfile\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import configparser\n",
        "#import mysql.connector\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3e7c5cad-7e70-46e9-9484-0209505e912e",
      "metadata": {
        "id": "3e7c5cad-7e70-46e9-9484-0209505e912e"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppressing the warnings\n",
        "warnings.filterwarnings('ignore') "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef408647-f9e9-448b-85ef-9f1772efb9c8",
      "metadata": {
        "id": "ef408647-f9e9-448b-85ef-9f1772efb9c8"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480c9a17-78f2-4d93-8404-cddb59dd43c9",
      "metadata": {
        "id": "480c9a17-78f2-4d93-8404-cddb59dd43c9"
      },
      "source": [
        "## General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4887335f-3783-4a95-94c1-79c422c428f4",
      "metadata": {
        "id": "4887335f-3783-4a95-94c1-79c422c428f4"
      },
      "outputs": [],
      "source": [
        "def check_folder(name_folder):            \n",
        "    #Creating folder if that doesn't exist\n",
        "    p = %pwd\n",
        "    p = p + f'/{name_folder}'\n",
        "    path = os.path.expanduser(p)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(\"{} created.\".format(path))\n",
        "        \n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "186417d1-fca8-4dcc-907e-da5c3591a4bf",
      "metadata": {
        "id": "186417d1-fca8-4dcc-907e-da5c3591a4bf"
      },
      "outputs": [],
      "source": [
        "def get_file_name(item_name, ext='*tar'):\n",
        "    \n",
        "    file_names = [f.name for f in internetarchive.get_files(item_name, glob_pattern= ext)]\n",
        "    \n",
        "    return file_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b8224599-ab0a-43db-a4af-63ceaaaf5fac",
      "metadata": {
        "id": "b8224599-ab0a-43db-a4af-63ceaaaf5fac",
        "outputId": "016fafe6-3ba1-4381-f866-ad1646a1348c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8c492f982ee5>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'{:.1f} GiB'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_bytes\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pieces'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
          ]
        }
      ],
      "source": [
        "def humanize(size_bytes):\n",
        "    KB = 1 << 10\n",
        "    MB = 1 << 20\n",
        "    GB = 1 << 30\n",
        "\n",
        "    if size_bytes < KB:\n",
        "        return '{} B'.format(size_bytes)\n",
        "    elif size_bytes < MB:\n",
        "        return '{:.1f} KiB'.format(size_bytes/KB)\n",
        "    elif size_bytes < GB:\n",
        "        return '{:.1f} MiB'.format(size_bytes/MB)\n",
        "    else:\n",
        "        return '{:.1f} GiB'.format(size_bytes/GB)\n",
        "\n",
        "print(info.num_pieces(), 'pieces')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a526cb-8bb0-4bdc-a383-b0e5841cab27",
      "metadata": {
        "id": "91a526cb-8bb0-4bdc-a383-b0e5841cab27"
      },
      "source": [
        "## Cleaning and Transfomations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12e37500-3043-48aa-8403-3ac8f136f9ef",
      "metadata": {
        "id": "12e37500-3043-48aa-8403-3ac8f136f9ef"
      },
      "outputs": [],
      "source": [
        "def get_data(file_path, file_name, word_bag = False, lang = False, db_update = False):\n",
        "    \n",
        "    #reading json\n",
        "    df = pd.read_json(file_path, lines=True, compression='gzip')\n",
        "    \n",
        "    #updating total tweets\n",
        "    if not update_db == False:\n",
        "        update_db(table = 'control', field = f\"count_total = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
        "    \n",
        "    #Filtering language if a language was sent\n",
        "    if not lang == False:\n",
        "        df = df[df.lang == lang]\n",
        "    \n",
        "    #taking away columns unnecessary\n",
        "    df = df.loc[:, ['created_at', 'text', 'entities']]\n",
        "    \n",
        "    #filtering Tweets using subject word bag if a word bag was sent\n",
        "    if not word_bag == False:\n",
        "        bow = '|'.join(word_bag) # bag of word\n",
        "        df = df[df['text'].str.contains(bow, case=False)]\n",
        "        \n",
        "        #updating filtered tweets\n",
        "        if not update_db == False:\n",
        "            update_db(table = 'control', field = f\"count_filtered = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3267d74a-1829-44cf-a53e-f0597320bf1d",
      "metadata": {
        "id": "3267d74a-1829-44cf-a53e-f0597320bf1d"
      },
      "source": [
        "## Control Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2af3344-6b81-4c18-a504-70041f78d9be",
      "metadata": {
        "id": "b2af3344-6b81-4c18-a504-70041f78d9be"
      },
      "source": [
        "### CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cc07a036-7be4-41c0-92d4-c4f489a34a71",
      "metadata": {
        "id": "cc07a036-7be4-41c0-92d4-c4f489a34a71"
      },
      "outputs": [],
      "source": [
        "def control_op_csv(control_file):\n",
        "\n",
        "    #if file doesn't exist I'll create it\n",
        "    if not os.path.exists(control_file):\n",
        "        with open(control_file, 'w', newline='') as control_csv:\n",
        "            writer = csv.DictWriter(control_csv, fieldnames=['name','datetime'])\n",
        "            writer.writeheader()\n",
        "        control_csv.close()\n",
        "        print(\"{} created.\".format(control_file))\n",
        "            \n",
        "    #Reading the control file\n",
        "    with open(control_file, 'r', newline='') as control_csv:\n",
        "        reader = csv.DictReader(control_csv)\n",
        "        reader_data = [r for r in reader]\n",
        "        control_csv.seek(0)\n",
        "    control_csv.close()\n",
        "    \n",
        "    return reader_data"
      ]
    },
    {
      "cell_type": "raw",
      "id": "857497e9-a0cf-4c51-ac3b-32b583edad21",
      "metadata": {
        "id": "857497e9-a0cf-4c51-ac3b-32b583edad21"
      },
      "source": [
        "path = check_folder('Downloads')\n",
        "\n",
        "#calling function for getting control file\n",
        "control_path = path + '/control_download.csv'    \n",
        "reader_data = control_op_csv(control_path)\n",
        "reader_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0502fcf3-2628-4ac4-b0d8-6145dac56060",
      "metadata": {
        "id": "0502fcf3-2628-4ac4-b0d8-6145dac56060"
      },
      "outputs": [],
      "source": [
        "def download_file_csv(item_name, file_name, download_folder = 'Downloads'):\n",
        "    \n",
        "    path = check_folder(download_folder)\n",
        "    \n",
        "    #calling function for getting control file\n",
        "    control_path = path + '/control_download.csv'    \n",
        "    reader_data = control_op_csv(control_path)\n",
        "    \n",
        "    #Getting Itens names\n",
        "    item = internetarchive.get_item(item_name)\n",
        "        \n",
        "    #openning control file to add file names on the list\n",
        "    with open(control_path, 'a', newline='') as control_download:\n",
        "        writer = csv.DictWriter(control_download, fieldnames=['name','datetime'])\n",
        "        \n",
        "        #check if the file has been downloaded before\n",
        "        if not any(row['name'] == file_name for row in reader_data):\n",
        "            \n",
        "            #downloading\n",
        "            r = item.download(\n",
        "                destdir=path,  # The directory to download files to\n",
        "                ignore_existing=True,  # Skip files that already exist locally\n",
        "                checksum=True,  # Skip files based on checksum\n",
        "                verbose=True,  # Print progress to stdout\n",
        "                retries=100,  # The number of times to retry on failed requests\n",
        "                no_directory=True,  # Download withtout the identifier\n",
        "                files = file_name)\n",
        "            \n",
        "            #Adding file name on control list\n",
        "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "            writer.writerow(row)\n",
        "            \n",
        "    control_download.close()\n",
        "    \n",
        "    path_file = path + f'/{file_name}'\n",
        "    \n",
        "    return path_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5ee9268d-0988-4248-9c72-bb22753af55b",
      "metadata": {
        "id": "5ee9268d-0988-4248-9c72-bb22753af55b"
      },
      "outputs": [],
      "source": [
        "def tar_file_csv(tar_file, file_name, extration_folder='Extraction'):\n",
        "\n",
        "    path = check_folder(extration_folder)\n",
        "    \n",
        "    #calling function for getting control file\n",
        "    control_path = path + '/control_extraction.csv'    \n",
        "    reader_data = control_op(control_path)\n",
        "\n",
        "    #openning control file to add file names on the list\n",
        "    with open(control_path, 'a', newline='') as control_tar:\n",
        "        writer = csv.DictWriter(control_tar, fieldnames=['name','datetime'])\n",
        "        \n",
        "        #check if the file has been extrated before\n",
        "        if not any(row['name'] == file_name for row in reader_data):\n",
        "\n",
        "            #Extracting file\n",
        "            tar_file.extract(file_name, path=path)\n",
        "\n",
        "            #Saving files name on control file\n",
        "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "            writer.writerow(row)\n",
        "            \n",
        "            path_file = path + f'/{file_name}'\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            path_file = False\n",
        "    \n",
        "    control_tar.close()\n",
        "    \n",
        "    \n",
        "    return path_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "530faf1b-3818-4358-90d1-2dc89ee170c8",
      "metadata": {
        "id": "530faf1b-3818-4358-90d1-2dc89ee170c8"
      },
      "source": [
        "### Mysql"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a6bc96-91f0-481b-b4bf-ab17cb3e0ba6",
      "metadata": {
        "id": "75a6bc96-91f0-481b-b4bf-ab17cb3e0ba6"
      },
      "source": [
        "#### Basic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b47408-deae-439f-84ee-ce0e09139d7f",
      "metadata": {
        "id": "51b47408-deae-439f-84ee-ce0e09139d7f"
      },
      "outputs": [],
      "source": [
        "def open_connection():\n",
        "\n",
        "    #getting configuration\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read('config.ini')\n",
        "\n",
        "    # Connect to Mysql\n",
        "    conn = mysql.connector.connect(\n",
        "        host = config['mysql']['host'],\n",
        "        user = config['mysql']['user'],\n",
        "        password = config['mysql']['password'],\n",
        "        database = config['mysql']['database']\n",
        "    )\n",
        "        \n",
        "    return conn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce187950-ba61-43da-b3b2-adcc26f74af7",
      "metadata": {
        "id": "ce187950-ba61-43da-b3b2-adcc26f74af7"
      },
      "outputs": [],
      "source": [
        "def update_db(table, field, condition):\n",
        "    \n",
        "    conn = open_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(f\"UPDATE {table} SET {field} WHERE {condition};\")\n",
        "\n",
        "    conn.commit()\n",
        "    cursor.close()\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74186bbb-ce88-48c8-aaf4-6dd0fbda7cd4",
      "metadata": {
        "id": "74186bbb-ce88-48c8-aaf4-6dd0fbda7cd4"
      },
      "outputs": [],
      "source": [
        "def query_db(query):\n",
        "    \n",
        "    conn = open_connection()\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    cursor.execute(query)\n",
        "\n",
        "    conn.commit()\n",
        "    cursor.close()\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2164244a-d491-42c2-bd19-2f62263ce64e",
      "metadata": {
        "id": "2164244a-d491-42c2-bd19-2f62263ce64e"
      },
      "outputs": [],
      "source": [
        "def read_db(query):\n",
        "    conn = open_connection()\n",
        "    \n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    \n",
        "    conn.close()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b340d245-7251-4b01-879e-187a50119290",
      "metadata": {
        "id": "b340d245-7251-4b01-879e-187a50119290"
      },
      "source": [
        "#### Specific operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d10c8ed-7d8a-4eae-84b3-2c11f7e62584",
      "metadata": {
        "id": "2d10c8ed-7d8a-4eae-84b3-2c11f7e62584"
      },
      "outputs": [],
      "source": [
        "def check_control_db(file_type, file_name = False):\n",
        "    \n",
        "    if file_name == False:\n",
        "        \n",
        "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}'\").to_dict(orient='records')\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}' AND name = '{file_name}'\").to_dict(orient='records')\n",
        "        \n",
        "        if len(check) > 0:\n",
        "            check = True\n",
        "        else:\n",
        "            check = False\n",
        "\n",
        "    return check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdffcd5-3416-4eed-8618-3950aeabef41",
      "metadata": {
        "id": "ebdffcd5-3416-4eed-8618-3950aeabef41",
        "outputId": "4bc20714-8d50-4323-faad-ec900d87d8d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'twitter-stream-20221001.tar'},\n",
              " {'name': 'twitter-stream-20221002.tar'},\n",
              " {'name': 'twitter-stream-20221003.tar'},\n",
              " {'name': 'twitter-stream-20221004.tar'},\n",
              " {'name': 'twitter-stream-20221005.tar'},\n",
              " {'name': 'twitter-stream-20221006.tar'},\n",
              " {'name': 'twitter-stream-20221007.tar'}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check_control_db(file_type = 'download')#, file_name = '20221001/20221001235900.json.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808f97b8-16b5-45c4-aa8e-5c2f16c7e90e",
      "metadata": {
        "id": "808f97b8-16b5-45c4-aa8e-5c2f16c7e90e"
      },
      "outputs": [],
      "source": [
        "def download_file_db(item_name, file_name, download_folder = 'Downloads', type_control = 'download'):\n",
        "    \n",
        "    path = check_folder(download_folder)\n",
        "    \n",
        "    #query to get information from Mysql about control\n",
        "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
        "                             \n",
        "    #check if the file has been unzipped before\n",
        "    if check == False:\n",
        "    \n",
        "        #getting Itens names\n",
        "        item = internetarchive.get_item(item_name)\n",
        "\n",
        "        #downloading\n",
        "        r = item.download(\n",
        "            destdir=path,  # The directory to download files to\n",
        "            ignore_existing=True,  # Skip files that already exist locally\n",
        "            checksum=True,  # Skip files based on checksum\n",
        "            verbose=False,  # Print progress to stdout\n",
        "            retries=100,  # The number of times to retry on failed requests\n",
        "            no_directory=True,  # Download withtout the identifier\n",
        "            files = file_name)\n",
        "        \n",
        "        #getting Metadata\n",
        "        metadata = list(filter(lambda p: p[\"name\"] == file_name, item.item_metadata['files']))\n",
        "\n",
        "        #Adding file name on control list\n",
        "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
        "        sz = metadata[0]['size']\n",
        "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}', '{dt}', '{type_control}', '{sz}')\")\n",
        "    \n",
        "        path_file = path + f'/{file_name}'\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        path_file = False   \n",
        "    \n",
        "    return path_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8a6aea-def3-47d2-b9dc-c85e10ba56f1",
      "metadata": {
        "id": "ff8a6aea-def3-47d2-b9dc-c85e10ba56f1"
      },
      "outputs": [],
      "source": [
        "def tar_file_db(tar_file, file_name, extration_folder='Extraction', type_control = 'extraction'):\n",
        "    \n",
        "    path = check_folder(extration_folder)\n",
        "    \n",
        "    #query to get information from Mysql about control\n",
        "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
        "    \n",
        "    \n",
        "    #check if the file has been unzipped before\n",
        "    if check == False:\n",
        "\n",
        "        #Extracting file\n",
        "        tar_file.extract(file_name, path=path)\n",
        "\n",
        "        #Adding file name on control list\n",
        "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
        "        sz = tar_file.getmember(name=file_name).size\n",
        "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}', '{dt}', '{type_control}', '{sz}')\")\n",
        "        \n",
        "        path_file = path + f'/{file_name}'\n",
        "    \n",
        "    else: #case the file has been unzipped\n",
        "            \n",
        "        path_file = False\n",
        "        \n",
        "    \n",
        "    return path_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ee5629-db75-4b84-9840-753eec3876dc",
      "metadata": {
        "id": "55ee5629-db75-4b84-9840-753eec3876dc"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd49266-a8dd-4b4d-9b82-af9f6b2b0492",
      "metadata": {
        "id": "ccd49266-a8dd-4b4d-9b82-af9f6b2b0492",
        "outputId": "52572e2e-1969-4f89-b3e2-7559d5a67b0e",
        "colab": {
          "referenced_widgets": [
            "05ecfe9713ae41ab836c09451b4f7813"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05ecfe9713ae41ab836c09451b4f7813",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Download progress:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "\n",
        "item_name = 'archiveteam-twitter-stream-2022-10'\n",
        "\n",
        "#Getting Itens names\n",
        "file_names = get_file_name(item_name = item_name)#, ext = '*jpg')\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    for dlf in tqdm(file_names, desc='Download progress'):\n",
        "        \n",
        "        #if x < 1: #for testing \n",
        "        #downloading the file\n",
        "        file_future = executor.submit(download_file_db, item_name=item_name, file_name=dlf)\n",
        "\n",
        "        if not file_future.result() == False:\n",
        "            \n",
        "            #Open tar file\n",
        "            tar = tarfile.open(file_future.result(), \"r\")\n",
        "\n",
        "            #Getting json files' names\n",
        "            tar_file_names = list(filter(lambda t: t.endswith('.json.gz'), tar.getnames()))\n",
        "\n",
        "            for tf in tqdm(tar_file_names, desc='Unzip progress'):\n",
        "\n",
        "                #call funcition to extract files\n",
        "                js_future = executor.submit(tar_file_db, tar_file=tar, file_name=tf)\n",
        "\n",
        "                #if the file has already been unzipped that part will be skipped\n",
        "                if not js_future.result() == False:\n",
        "\n",
        "                    #Getting, cleaning and storing Data\n",
        "                    bow = ['vaccination', 'vaccines', 'vaccine']\n",
        "                    df = get_data(file_path = js_future.result(), file_name = tf, lang = 'en', word_bag = bow, db_update = True)\n",
        "\n",
        "                    if not df.empty:\n",
        "\n",
        "                        conn = open_connection()\n",
        "                        cursor = conn.cursor()\n",
        "\n",
        "                        for index, row in df.iterrows():\n",
        "                            created_at = row['created_at']\n",
        "                            text = row['text']\n",
        "                            entities = json.dumps(row['entities'])\n",
        "\n",
        "                            sql = \"INSERT INTO tweets (created_at, text, entities) VALUES (%s, %s, %s)\"\n",
        "                            cursor.execute(sql, (created_at, text, entities))\n",
        "\n",
        "                            conn.commit()\n",
        "\n",
        "                        cursor.close()\n",
        "                        conn.close()\n",
        "\n",
        "                    #deleting json file after processing\n",
        "                    os.remove(js_future.result())\n",
        "\n",
        "            #closing tar file\n",
        "            tar.close()\n",
        "\n",
        "            #x =+ 1\n",
        "\n",
        "            #removing file downloaded\n",
        "            os.remove(file_future.result())\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "45b19d87-4fda-4a4b-abe8-27bcebe6fb22",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "45b19d87-4fda-4a4b-abe8-27bcebe6fb22"
      },
      "source": [
        "item_name = 'archiveteam-twitter-stream-2022-10'\n",
        "\n",
        "#Getting Itens names\n",
        "file_names = get_file_name(item_name = item_name)#, ext = '*jpg')\n",
        "\n",
        "x = 0 #for testing \n",
        "\n",
        "for dlf in tqdm(file_names, desc='Download progress'):\n",
        "    \n",
        "    #if x < 1: #for testing \n",
        "    #downloading the file\n",
        "    file = download_file_db(item_name = item_name, file_name = dlf)\n",
        "    \n",
        "    if not file == False:\n",
        "\n",
        "        #Open tar file\n",
        "        tar = tarfile.open(file, \"r\")\n",
        "\n",
        "        #Getting json files' names\n",
        "        tar_file_names = list(filter(lambda t: t.endswith('.json.gz'), tar.getnames()))\n",
        "\n",
        "        for tf in tqdm(tar_file_names, desc='Unzip progress'):\n",
        "\n",
        "            #call funcition to extract files\n",
        "            js = tar_file_db(tar_file = tar, file_name = tf)\n",
        "\n",
        "            #if the file has already been unzipped that part will be skipped\n",
        "            if not js == False:\n",
        "\n",
        "                #Getting, cleaning and storing Data\n",
        "                bow = ['vaccination', 'vaccines', 'vaccine']\n",
        "                df = get_data(file_path = js, file_name = tf, lang = 'en', word_bag = bow, db_update = True)\n",
        "\n",
        "                if not df.empty:\n",
        "\n",
        "                    conn = open_connection()\n",
        "                    cursor = conn.cursor()\n",
        "\n",
        "                    for index, row in df.iterrows():\n",
        "                        created_at = row['created_at']\n",
        "                        text = row['text']\n",
        "                        entities = json.dumps(row['entities'])\n",
        "\n",
        "                        sql = \"INSERT INTO tweets (created_at, text, entities) VALUES (%s, %s, %s)\"\n",
        "                        cursor.execute(sql, (created_at, text, entities))\n",
        "\n",
        "                        conn.commit()\n",
        "\n",
        "                    cursor.close()\n",
        "                    conn.close()\n",
        "\n",
        "                #deleting json file after processing\n",
        "                os.remove(js)\n",
        "\n",
        "        #closing tar file\n",
        "        tar.close()\n",
        "\n",
        "        #x =+ 1\n",
        "\n",
        "        #removing file downloaded\n",
        "        os.remove(file)\n",
        "        \n",
        "    #"
      ]
    },
    {
      "cell_type": "raw",
      "id": "243b4a23-11cf-44c6-94d7-fe8e21d65865",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "243b4a23-11cf-44c6-94d7-fe8e21d65865"
      },
      "source": [
        "conn = open_connection()\n",
        "cursor = conn.cursor()\n",
        "\n",
        "now = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
        "\n",
        "cursor.execute(f\"INSERT INTO control (name, datetime, type) VALUES ('Teste', '{now}', 'download')\")\n",
        "\n",
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9f18fa-2d29-4789-b6d2-6f59f7649fd0",
      "metadata": {
        "id": "cd9f18fa-2d29-4789-b6d2-6f59f7649fd0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95132611-4008-4a35-9de6-05c7f4e766e7",
      "metadata": {
        "id": "95132611-4008-4a35-9de6-05c7f4e766e7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a6291b-6142-49be-9799-d7eab06dc00c",
      "metadata": {
        "id": "13a6291b-6142-49be-9799-d7eab06dc00c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2542e50a-a91f-4c49-9889-0fb34e855bcf",
      "metadata": {
        "id": "2542e50a-a91f-4c49-9889-0fb34e855bcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf51442-8748-4e3c-b482-5b232b09dcd8",
      "metadata": {
        "id": "2bf51442-8748-4e3c-b482-5b232b09dcd8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243e2b02-ac8f-481d-a2b9-8781bcb5085e",
      "metadata": {
        "id": "243e2b02-ac8f-481d-a2b9-8781bcb5085e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6985f02c-fd5f-417f-a3a3-c90b42a7f823",
      "metadata": {
        "id": "6985f02c-fd5f-417f-a3a3-c90b42a7f823"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a95354-d191-4f9e-8c54-f0862a54bde4",
      "metadata": {
        "id": "44a95354-d191-4f9e-8c54-f0862a54bde4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc-autonumbering": true,
    "toc-showcode": true,
    "toc-showmarkdowntxt": true,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
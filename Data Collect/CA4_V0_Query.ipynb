{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfcb2a1-c76f-4c24-957e-9369c40ed719",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29dd464e-a654-4118-8cfb-743f936d12c8",
   "metadata": {},
   "source": [
    "!pip install -q internetarchive\n",
    "!pip install -q mysql-connector-python\n",
    "!pip install -q libtorrent\n",
    "!pip install -q hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d93c2-4b48-4cf7-a445-89db0a09a599",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b36d8b9-4b17-4bea-bbfd-a4beb7244d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import internetarchive\n",
    "import os\n",
    "import tarfile\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "#Hadoop\n",
    "from hdfs import InsecureClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7c5cad-7e70-46e9-9484-0209505e912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing the warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef408647-f9e9-448b-85ef-9f1772efb9c8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c9a17-78f2-4d93-8404-cddb59dd43c9",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4887335f-3783-4a95-94c1-79c422c428f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(name_folder):            \n",
    "    #Creating folder if that doesn't exist\n",
    "    p = %pwd\n",
    "    p = p + f'/{name_folder}'\n",
    "    path = os.path.expanduser(p)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(\"{} created.\".format(path))\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "186417d1-fca8-4dcc-907e-da5c3591a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(item_name, ext='*tar'):\n",
    "    \n",
    "    file_names = [f.name for f in internetarchive.get_files(item_name, glob_pattern= ext)]\n",
    "    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8224599-ab0a-43db-a4af-63ceaaaf5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize(size_bytes):\n",
    "    KB = 1 << 10\n",
    "    MB = 1 << 20\n",
    "    GB = 1 << 30\n",
    "\n",
    "    if size_bytes < KB:\n",
    "        return '{} B'.format(size_bytes)\n",
    "    elif size_bytes < MB:\n",
    "        return '{:.2f} KiB'.format(size_bytes/KB)\n",
    "    elif size_bytes < GB:\n",
    "        return '{:.2f} MiB'.format(size_bytes/MB)\n",
    "    else:\n",
    "        return '{:.2f} GiB'.format(size_bytes/GB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a526cb-8bb0-4bdc-a383-b0e5841cab27",
   "metadata": {},
   "source": [
    "## Cleaning and Transfomations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e37500-3043-48aa-8403-3ac8f136f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path, file_name, word_bag = False, lang = False, db_update = False):\n",
    "    \n",
    "    #reading json\n",
    "    df = pd.read_json(file_path, lines=True, compression='gzip')\n",
    "    \n",
    "    #updating total tweets\n",
    "    if not update_db == False:\n",
    "        update_db(table = 'control', field = f\"count_total = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
    "    \n",
    "    #Filtering language if a language was sent\n",
    "    if not lang == False:\n",
    "        df = df[df.lang == lang]\n",
    "    \n",
    "    #taking away columns unnecessary\n",
    "    df = df.loc[:, ['created_at', 'text', 'entities']]\n",
    "    \n",
    "    #filtering Tweets using subject word bag if a word bag was sent\n",
    "    if not word_bag == False:\n",
    "        bow = '|'.join(word_bag) # bag of word\n",
    "        df = df[df['text'].str.contains(bow, case=False)]\n",
    "        \n",
    "        #updating filtered tweets\n",
    "        if not update_db == False:\n",
    "            update_db(table = 'control', field = f\"count_filtered = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267d74a-1829-44cf-a53e-f0597320bf1d",
   "metadata": {},
   "source": [
    "## Control Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af3344-6b81-4c18-a504-70041f78d9be",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc07a036-7be4-41c0-92d4-c4f489a34a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_op_csv(control_file):\n",
    "\n",
    "    #if file doesn't exist I'll create it\n",
    "    if not os.path.exists(control_file):\n",
    "        with open(control_file, 'w', newline='') as control_csv:\n",
    "            writer = csv.DictWriter(control_csv, fieldnames=['name','datetime'])\n",
    "            writer.writeheader()\n",
    "        control_csv.close()\n",
    "        print(\"{} created.\".format(control_file))\n",
    "            \n",
    "    #Reading the control file\n",
    "    with open(control_file, 'r', newline='') as control_csv:\n",
    "        reader = csv.DictReader(control_csv)\n",
    "        reader_data = [r for r in reader]\n",
    "        control_csv.seek(0)\n",
    "    control_csv.close()\n",
    "    \n",
    "    return reader_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "857497e9-a0cf-4c51-ac3b-32b583edad21",
   "metadata": {},
   "source": [
    "path = check_folder('Downloads')\n",
    "\n",
    "#calling function for getting control file\n",
    "control_path = path + '/control_download.csv'    \n",
    "reader_data = control_op_csv(control_path)\n",
    "reader_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0502fcf3-2628-4ac4-b0d8-6145dac56060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_csv(item_name, file_name, download_folder = 'Downloads'):\n",
    "    \n",
    "    path = check_folder(download_folder)\n",
    "    \n",
    "    #calling function for getting control file\n",
    "    control_path = path + '/control_download.csv'    \n",
    "    reader_data = control_op_csv(control_path)\n",
    "    \n",
    "    #Getting Itens names\n",
    "    item = internetarchive.get_item(item_name)\n",
    "        \n",
    "    #openning control file to add file names on the list\n",
    "    with open(control_path, 'a', newline='') as control_download:\n",
    "        writer = csv.DictWriter(control_download, fieldnames=['name','datetime'])\n",
    "        \n",
    "        #check if the file has been downloaded before\n",
    "        if not any(row['name'] == file_name for row in reader_data):\n",
    "            \n",
    "            #downloading\n",
    "            r = item.download(\n",
    "                destdir=path,  # The directory to download files to\n",
    "                ignore_existing=True,  # Skip files that already exist locally\n",
    "                checksum=True,  # Skip files based on checksum\n",
    "                verbose=True,  # Print progress to stdout\n",
    "                retries=100,  # The number of times to retry on failed requests\n",
    "                no_directory=True,  # Download withtout the identifier\n",
    "                files = file_name)\n",
    "            \n",
    "            #Adding file name on control list\n",
    "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    control_download.close()\n",
    "    \n",
    "    path_file = path + f'/{file_name}'\n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ee9268d-0988-4248-9c72-bb22753af55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_file_csv(tar_file, file_name, extration_folder='Extraction'):\n",
    "\n",
    "    path = check_folder(extration_folder)\n",
    "    \n",
    "    #calling function for getting control file\n",
    "    control_path = path + '/control_extraction.csv'    \n",
    "    reader_data = control_op(control_path)\n",
    "\n",
    "    #openning control file to add file names on the list\n",
    "    with open(control_path, 'a', newline='') as control_tar:\n",
    "        writer = csv.DictWriter(control_tar, fieldnames=['name','datetime'])\n",
    "        \n",
    "        #check if the file has been extrated before\n",
    "        if not any(row['name'] == file_name for row in reader_data):\n",
    "\n",
    "            #Extracting file\n",
    "            tar_file.extract(file_name, path=path)\n",
    "\n",
    "            #Saving files name on control file\n",
    "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            path_file = path + f'/{file_name}'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            path_file = False\n",
    "    \n",
    "    control_tar.close()\n",
    "    \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530faf1b-3818-4358-90d1-2dc89ee170c8",
   "metadata": {},
   "source": [
    "### Mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6bc96-91f0-481b-b4bf-ab17cb3e0ba6",
   "metadata": {},
   "source": [
    "#### Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51b47408-deae-439f-84ee-ce0e09139d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_connection():\n",
    "\n",
    "    #getting configuration\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "\n",
    "    # Connect to Mysql\n",
    "    conn = mysql.connector.connect(\n",
    "        host = config['mysql']['host'],\n",
    "        user = config['mysql']['user'],\n",
    "        password = config['mysql']['password'],\n",
    "        database = config['mysql']['database']\n",
    "    )\n",
    "        \n",
    "    return conn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce187950-ba61-43da-b3b2-adcc26f74af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(table, field, condition):\n",
    "    \n",
    "    conn = open_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"UPDATE {table} SET {field} WHERE {condition};\")\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74186bbb-ce88-48c8-aaf4-6dd0fbda7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query):\n",
    "    \n",
    "    conn = open_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(query)\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2164244a-d491-42c2-bd19-2f62263ce64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db(query):\n",
    "    conn = open_connection()\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340d245-7251-4b01-879e-187a50119290",
   "metadata": {},
   "source": [
    "#### Specific operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d10c8ed-7d8a-4eae-84b3-2c11f7e62584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_control_db(file_type, file_name = False):\n",
    "    \n",
    "    if file_name == False:\n",
    "        \n",
    "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}'\").to_dict(orient='records')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}' AND name = '{file_name}'\"\n",
    "                       ).to_dict(orient='records')\n",
    "        \n",
    "        if len(check) > 0:\n",
    "            check = True\n",
    "        else:\n",
    "            check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "808f97b8-16b5-45c4-aa8e-5c2f16c7e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_db(item_name, file_name, download_folder = 'Downloads', type_control = 'download'):\n",
    "    \n",
    "    path = check_folder(download_folder)\n",
    "    \n",
    "    #query to get information from Mysql about control\n",
    "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
    "                             \n",
    "    #check if the file has been unzipped before\n",
    "    if check == False:\n",
    "    \n",
    "        #getting Itens names\n",
    "        item = internetarchive.get_item(item_name)\n",
    "\n",
    "        #downloading\n",
    "        r = item.download(\n",
    "            destdir=path,  # The directory to download files to\n",
    "            ignore_existing=True,  # Skip files that already exist locally\n",
    "            checksum=True,  # Skip files based on checksum\n",
    "            verbose=False,  # Print progress to stdout\n",
    "            retries=100,  # The number of times to retry on failed requests\n",
    "            no_directory=True,  # Download withtout the identifier\n",
    "            files = file_name)\n",
    "        \n",
    "        #getting Metadata\n",
    "        metadata = list(filter(lambda p: p[\"name\"] == file_name, item.item_metadata['files']))\n",
    "\n",
    "        #Adding file name on control list\n",
    "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        sz = metadata[0]['size']\n",
    "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}','{dt}','{type_control}','{sz}')\")\n",
    "    \n",
    "        path_file = path + f'/{file_name}'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        path_file = False   \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff8a6aea-def3-47d2-b9dc-c85e10ba56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_file_db(tar_file, file_name, extration_folder='Extraction', type_control = 'extraction'):\n",
    "    \n",
    "    path = check_folder(extration_folder)\n",
    "    \n",
    "    #query to get information from Mysql about control\n",
    "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
    "    \n",
    "    \n",
    "    #check if the file has been unzipped before\n",
    "    if check == False:\n",
    "\n",
    "        #Extracting file\n",
    "        tar_file.extract(file_name, path=path)\n",
    "\n",
    "        #Adding file name on control list\n",
    "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        sz = tar_file.getmember(name=file_name).size\n",
    "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}', '{dt}', '{type_control}', '{sz}')\")\n",
    "        \n",
    "        path_file = path + f'/{file_name}'\n",
    "    \n",
    "    else: #case the file has been unzipped\n",
    "            \n",
    "        path_file = False\n",
    "        \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e7af889-caee-42a5-8fd6-af183dbf0dad",
   "metadata": {},
   "source": [
    "read_db(\"SELECT * FROM control WHERE type = 'download'AND name LIKE 'twitter-stream-202208%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee5629-db75-4b84-9840-753eec3876dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checking Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d94a2-200a-4d49-9f8c-0ab5eeb45ee1",
   "metadata": {},
   "source": [
    "## Getting info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45e74d56-fa23-4898-a516-d426a4faf094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = read_db(\"SELECT * FROM control WHERE type = 'extraction'\")\n",
    "down_df = check_control_db(file_type = 'download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eec1e704-d36d-437a-9f73-facb9c4a1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'] = df['name'].str[:8].apply(lambda x: datetime.strptime(x, '%Y%m%d').strftime('%Y-%m-%d'))\n",
    "df['data'] = pd.to_datetime(df['data'])\n",
    "df['size'] = df['size'].astype(float)\n",
    "df['count_total'] = df['count_total'].astype(float)\n",
    "df['count_filtered'] = df['count_filtered'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e6d0deb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idnew_table</th>\n",
       "      <th>name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>count_total</th>\n",
       "      <th>count_filtered</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>660</td>\n",
       "      <td>20221001/20221001235900.json.gz</td>\n",
       "      <td>2023-04-30 02:06:52</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1734723.0</td>\n",
       "      <td>2558.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>661</td>\n",
       "      <td>20221001/20221001235500.json.gz</td>\n",
       "      <td>2023-04-30 02:06:53</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1740434.0</td>\n",
       "      <td>2558.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>662</td>\n",
       "      <td>20221001/20221001235300.json.gz</td>\n",
       "      <td>2023-04-30 02:06:54</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1738464.0</td>\n",
       "      <td>2539.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>663</td>\n",
       "      <td>20221001/20221001235600.json.gz</td>\n",
       "      <td>2023-04-30 02:06:54</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1844137.0</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>664</td>\n",
       "      <td>20221001/20221001235700.json.gz</td>\n",
       "      <td>2023-04-30 02:06:55</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1720647.0</td>\n",
       "      <td>2565.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514978</th>\n",
       "      <td>516060</td>\n",
       "      <td>20220131/20220131000200.json.gz</td>\n",
       "      <td>2023-05-07 19:42:34</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1651805.0</td>\n",
       "      <td>2453.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514979</th>\n",
       "      <td>516061</td>\n",
       "      <td>20220131/20220131001500.json.gz</td>\n",
       "      <td>2023-05-07 19:42:35</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1511317.0</td>\n",
       "      <td>2317.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514980</th>\n",
       "      <td>516062</td>\n",
       "      <td>20220131/20220131000300.json.gz</td>\n",
       "      <td>2023-05-07 19:42:36</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1632604.0</td>\n",
       "      <td>2432.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514981</th>\n",
       "      <td>516063</td>\n",
       "      <td>20220131/20220131000000.json.gz</td>\n",
       "      <td>2023-05-07 19:42:37</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1663402.0</td>\n",
       "      <td>2568.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514982</th>\n",
       "      <td>516064</td>\n",
       "      <td>20220131/20220131000100.json.gz</td>\n",
       "      <td>2023-05-07 19:42:38</td>\n",
       "      <td>extraction</td>\n",
       "      <td>1646000.0</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>514983 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idnew_table                             name            datetime  \\\n",
       "0               660  20221001/20221001235900.json.gz 2023-04-30 02:06:52   \n",
       "1               661  20221001/20221001235500.json.gz 2023-04-30 02:06:53   \n",
       "2               662  20221001/20221001235300.json.gz 2023-04-30 02:06:54   \n",
       "3               663  20221001/20221001235600.json.gz 2023-04-30 02:06:54   \n",
       "4               664  20221001/20221001235700.json.gz 2023-04-30 02:06:55   \n",
       "...             ...                              ...                 ...   \n",
       "514978       516060  20220131/20220131000200.json.gz 2023-05-07 19:42:34   \n",
       "514979       516061  20220131/20220131001500.json.gz 2023-05-07 19:42:35   \n",
       "514980       516062  20220131/20220131000300.json.gz 2023-05-07 19:42:36   \n",
       "514981       516063  20220131/20220131000000.json.gz 2023-05-07 19:42:37   \n",
       "514982       516064  20220131/20220131000100.json.gz 2023-05-07 19:42:38   \n",
       "\n",
       "              type       size  count_total  count_filtered       data  \n",
       "0       extraction  1734723.0       2558.0             0.0 2022-10-01  \n",
       "1       extraction  1740434.0       2558.0             0.0 2022-10-01  \n",
       "2       extraction  1738464.0       2539.0             0.0 2022-10-01  \n",
       "3       extraction  1844137.0       2753.0             0.0 2022-10-01  \n",
       "4       extraction  1720647.0       2565.0             1.0 2022-10-01  \n",
       "...            ...        ...          ...             ...        ...  \n",
       "514978  extraction  1651805.0       2453.0             3.0 2022-01-31  \n",
       "514979  extraction  1511317.0       2317.0             4.0 2022-01-31  \n",
       "514980  extraction  1632604.0       2432.0             3.0 2022-01-31  \n",
       "514981  extraction  1663402.0       2568.0             2.0 2022-01-31  \n",
       "514982  extraction  1646000.0       2458.0             3.0 2022-01-31  \n",
       "\n",
       "[514983 rows x 8 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc89d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean:2.75 days per month\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupby(pd.Grouper(key='data', freq='M'))['datetime'].agg(['min', 'max'])\n",
    "\n",
    "\n",
    "grouped_df['diff_days'] = (grouped_df['max'] - grouped_df['min']).dt.days\n",
    "\n",
    "\n",
    "print(f\"\\n Mean:{grouped_df['diff_days'].mean()} days per month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44ed0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean: 47.02739726027397 minutes per day\n",
      "\n",
      " Max: 1244 minutes per day\n",
      "\n",
      " Min: 14 minutes per day\n",
      "\n",
      " Diff Total: 7 days 23:07:39\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupby(pd.Grouper(key='data', freq='D'))['datetime'].agg(['min', 'max'])\n",
    "\n",
    "\n",
    "grouped_df['diff_min'] = (grouped_df['max'] - grouped_df['min']).dt.seconds // 60\n",
    "\n",
    "days_total = (grouped_df['max'].max() - grouped_df['min'].min())\n",
    "\n",
    "print(f\"\\n Mean: {grouped_df['diff_min'].mean()} minutes per day\")\n",
    "print(f\"\\n Max: {grouped_df['diff_min'].max()} minutes per day\")\n",
    "print(f\"\\n Min: {grouped_df['diff_min'].min()} minutes per day\")\n",
    "\n",
    "print(f\"\\n Diff Total: {days_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2764c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,742 tweets per file\n",
      "1,411 json file per day\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df['count_total'].mean():,.0f} tweets per file\")\n",
    "\n",
    "min_day = (df.groupby(pd.Grouper(key='data', freq='D'))['data'].count()).mean()\n",
    "\n",
    "print(f\"{min_day:,.0f} json file per day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "565029d7-5242-4f98-a47d-d723a1949e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of days: 365 \n",
      "\n",
      "Total Downloaded: 917.96 GiB\n",
      "An average of 2.51 GiB per day downloaded \n",
      "\n",
      "Total of Tweets: 1,412,214,396.0\n",
      "An average of 3,869,081 tweets downloaded per day \n",
      "\n",
      "Tweets filtered: 763,267.0\n",
      "An average of 2,091 tweets filtered per day \n",
      "\n"
     ]
    }
   ],
   "source": [
    "donwloaded = df.groupby(pd.Grouper(key='data', freq='M'))['size'].sum()\n",
    "print(f\"Total of days: {len(down_df)} \\n\")\n",
    "\n",
    "print(f\"Total Downloaded: {humanize(donwloaded.sum())}\")\n",
    "print(f\"An average of {humanize(donwloaded.sum()/len(down_df))} per day downloaded \\n\")\n",
    "\n",
    "print(\"Total of Tweets: {:,}\".format(df['count_total'].sum()))\n",
    "print(f\"An average of {df['count_total'].sum()/len(down_df):,.0f} tweets downloaded per day \\n\")\n",
    "\n",
    "print(\"Tweets filtered: {:,}\".format(df['count_filtered'].sum()))\n",
    "print(f\"An average of {df['count_filtered'].sum()/len(down_df):,.0f} tweets filtered per day \\n\")\n",
    "\n",
    "\n",
    "#deleting df\n",
    "del down_df, donwloaded, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2465e-7322-491a-bc76-2a532015a6f5",
   "metadata": {},
   "source": [
    "## Checking Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fcb9042-08dd-4158-b997-3af1ed4790b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_control = read_db(f\"SELECT name FROM control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4151ad9-6d6b-43db-babc-1524cfae5ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't find duplicated\n"
     ]
    }
   ],
   "source": [
    "duplicated = names_control.duplicated(['name'])\n",
    "df_duplicated = names_control.loc[duplicated, :]\n",
    "if len(df_duplicated)> 0 :\n",
    "    print(f'Find {len(df_duplicated)} duplicated')\n",
    "    print(df_duplicados)\n",
    "else:\n",
    "    print(\"Don't find duplicated\") \n",
    "    \n",
    "    \n",
    "#deleting df\n",
    "del duplicated, df_duplicated, names_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8bff9-2b4f-4a04-af4c-83a1d3aa9ec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing to Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bf59fce-c6b2-4dfd-8c83-d1c0e199373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysql_to_hdfs(path, mysql_table, hdfs_folder = '/CA4' ,split_by_month=False, date_col='created_at'):\n",
    "\n",
    "    path = check_folder(path)\n",
    "\n",
    "    #Loading data from MySQL database\n",
    "    df = read_db(f\"SELECT * FROM {mysql_table}\")\n",
    "    \n",
    "    if split_by_month == True:\n",
    "        \n",
    "        #Adding month\n",
    "        df[date_col] = pd.to_datetime(df[date_col]).dt.tz_localize('UTC')\n",
    "        df['month'] = df[date_col].dt.month\n",
    "\n",
    "        # Splitting DataFrame by month and saving in parquet files \n",
    "        for month in range(1, 13):\n",
    "            month_df = df[df['month'] == month]\n",
    "            month_df.to_parquet(f\"{path}/{mysql_table}_{month}.parquet\")\n",
    "            print(f\"{mysql_table}_{month}.parquet >>> SAVED AT >>> {path}\")\n",
    "            \n",
    "    else:\n",
    "        df.to_parquet(f\"{path}/{mysql_table}.parquet\")\n",
    "    \n",
    "    # Create a client\n",
    "    client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "    # Copy files to hdfs\n",
    "    client.upload(hdfs_folder, path)\n",
    "    print(f\"{path} >>> UPLOADED AT >>> {hdfs_folder}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b2599-f576-4475-b4f0-598f571b0b3e",
   "metadata": {},
   "source": [
    "## Control Table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f17edd8",
   "metadata": {},
   "source": [
    "mysql_to_hdfs(path = 'data/control', \n",
    "              mysql_table = 'control',\n",
    "              hdfs_folder = '/CA4',\n",
    "              split_by_month = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9436713-8732-474b-b2cd-ea2316047637",
   "metadata": {},
   "source": [
    "## Tweets Table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c18837f",
   "metadata": {},
   "source": [
    "mysql_to_hdfs(path = 'data/tweets', \n",
    "              mysql_table = 'tweets',\n",
    "              hdfs_folder = '/CA4',\n",
    "              split_by_month = True,\n",
    "              date_col='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a047f13-0ea3-41a4-8356-2adfff2a4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

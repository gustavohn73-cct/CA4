{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfcb2a1-c76f-4c24-957e-9369c40ed719",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29dd464e-a654-4118-8cfb-743f936d12c8",
   "metadata": {},
   "source": [
    "!pip install -q internetarchive\n",
    "!pip install -q mysql-connector-python\n",
    "!pip install -q libtorrent\n",
    "!pip install -q hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d93c2-4b48-4cf7-a445-89db0a09a599",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b36d8b9-4b17-4bea-bbfd-a4beb7244d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 48288)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/anaconda3/lib/python3.9/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import internetarchive\n",
    "import os\n",
    "import tarfile\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "#Hadoop\n",
    "from hdfs import InsecureClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7c5cad-7e70-46e9-9484-0209505e912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing the warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef408647-f9e9-448b-85ef-9f1772efb9c8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c9a17-78f2-4d93-8404-cddb59dd43c9",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4887335f-3783-4a95-94c1-79c422c428f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(name_folder):            \n",
    "    #Creating folder if that doesn't exist\n",
    "    p = %pwd\n",
    "    p = p + f'/{name_folder}'\n",
    "    path = os.path.expanduser(p)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(\"{} created.\".format(path))\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186417d1-fca8-4dcc-907e-da5c3591a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(item_name, ext='*tar'):\n",
    "    \n",
    "    file_names = [f.name for f in internetarchive.get_files(item_name, glob_pattern= ext)]\n",
    "    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8224599-ab0a-43db-a4af-63ceaaaf5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize(size_bytes):\n",
    "    KB = 1 << 10\n",
    "    MB = 1 << 20\n",
    "    GB = 1 << 30\n",
    "\n",
    "    if size_bytes < KB:\n",
    "        return '{} B'.format(size_bytes)\n",
    "    elif size_bytes < MB:\n",
    "        return '{:.2f} KiB'.format(size_bytes/KB)\n",
    "    elif size_bytes < GB:\n",
    "        return '{:.2f} MiB'.format(size_bytes/MB)\n",
    "    else:\n",
    "        return '{:.2f} GiB'.format(size_bytes/GB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a526cb-8bb0-4bdc-a383-b0e5841cab27",
   "metadata": {},
   "source": [
    "## Cleaning and Transfomations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e37500-3043-48aa-8403-3ac8f136f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path, file_name, word_bag = False, lang = False, db_update = False):\n",
    "    \n",
    "    #reading json\n",
    "    df = pd.read_json(file_path, lines=True, compression='gzip')\n",
    "    \n",
    "    #updating total tweets\n",
    "    if not update_db == False:\n",
    "        update_db(table = 'control', field = f\"count_total = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
    "    \n",
    "    #Filtering language if a language was sent\n",
    "    if not lang == False:\n",
    "        df = df[df.lang == lang]\n",
    "    \n",
    "    #taking away columns unnecessary\n",
    "    df = df.loc[:, ['created_at', 'text', 'entities']]\n",
    "    \n",
    "    #filtering Tweets using subject word bag if a word bag was sent\n",
    "    if not word_bag == False:\n",
    "        bow = '|'.join(word_bag) # bag of word\n",
    "        df = df[df['text'].str.contains(bow, case=False)]\n",
    "        \n",
    "        #updating filtered tweets\n",
    "        if not update_db == False:\n",
    "            update_db(table = 'control', field = f\"count_filtered = '{len(df)}'\", condition = f\"name = '{file_name}'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267d74a-1829-44cf-a53e-f0597320bf1d",
   "metadata": {},
   "source": [
    "## Control Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af3344-6b81-4c18-a504-70041f78d9be",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc07a036-7be4-41c0-92d4-c4f489a34a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_op_csv(control_file):\n",
    "\n",
    "    #if file doesn't exist I'll create it\n",
    "    if not os.path.exists(control_file):\n",
    "        with open(control_file, 'w', newline='') as control_csv:\n",
    "            writer = csv.DictWriter(control_csv, fieldnames=['name','datetime'])\n",
    "            writer.writeheader()\n",
    "        control_csv.close()\n",
    "        print(\"{} created.\".format(control_file))\n",
    "            \n",
    "    #Reading the control file\n",
    "    with open(control_file, 'r', newline='') as control_csv:\n",
    "        reader = csv.DictReader(control_csv)\n",
    "        reader_data = [r for r in reader]\n",
    "        control_csv.seek(0)\n",
    "    control_csv.close()\n",
    "    \n",
    "    return reader_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "857497e9-a0cf-4c51-ac3b-32b583edad21",
   "metadata": {},
   "source": [
    "path = check_folder('Downloads')\n",
    "\n",
    "#calling function for getting control file\n",
    "control_path = path + '/control_download.csv'    \n",
    "reader_data = control_op_csv(control_path)\n",
    "reader_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0502fcf3-2628-4ac4-b0d8-6145dac56060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_csv(item_name, file_name, download_folder = 'Downloads'):\n",
    "    \n",
    "    path = check_folder(download_folder)\n",
    "    \n",
    "    #calling function for getting control file\n",
    "    control_path = path + '/control_download.csv'    \n",
    "    reader_data = control_op_csv(control_path)\n",
    "    \n",
    "    #Getting Itens names\n",
    "    item = internetarchive.get_item(item_name)\n",
    "        \n",
    "    #openning control file to add file names on the list\n",
    "    with open(control_path, 'a', newline='') as control_download:\n",
    "        writer = csv.DictWriter(control_download, fieldnames=['name','datetime'])\n",
    "        \n",
    "        #check if the file has been downloaded before\n",
    "        if not any(row['name'] == file_name for row in reader_data):\n",
    "            \n",
    "            #downloading\n",
    "            r = item.download(\n",
    "                destdir=path,  # The directory to download files to\n",
    "                ignore_existing=True,  # Skip files that already exist locally\n",
    "                checksum=True,  # Skip files based on checksum\n",
    "                verbose=True,  # Print progress to stdout\n",
    "                retries=100,  # The number of times to retry on failed requests\n",
    "                no_directory=True,  # Download withtout the identifier\n",
    "                files = file_name)\n",
    "            \n",
    "            #Adding file name on control list\n",
    "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    control_download.close()\n",
    "    \n",
    "    path_file = path + f'/{file_name}'\n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee9268d-0988-4248-9c72-bb22753af55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_file_csv(tar_file, file_name, extration_folder='Extraction'):\n",
    "\n",
    "    path = check_folder(extration_folder)\n",
    "    \n",
    "    #calling function for getting control file\n",
    "    control_path = path + '/control_extraction.csv'    \n",
    "    reader_data = control_op(control_path)\n",
    "\n",
    "    #openning control file to add file names on the list\n",
    "    with open(control_path, 'a', newline='') as control_tar:\n",
    "        writer = csv.DictWriter(control_tar, fieldnames=['name','datetime'])\n",
    "        \n",
    "        #check if the file has been extrated before\n",
    "        if not any(row['name'] == file_name for row in reader_data):\n",
    "\n",
    "            #Extracting file\n",
    "            tar_file.extract(file_name, path=path)\n",
    "\n",
    "            #Saving files name on control file\n",
    "            row = {'name' : file_name, 'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            path_file = path + f'/{file_name}'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            path_file = False\n",
    "    \n",
    "    control_tar.close()\n",
    "    \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530faf1b-3818-4358-90d1-2dc89ee170c8",
   "metadata": {},
   "source": [
    "### Mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6bc96-91f0-481b-b4bf-ab17cb3e0ba6",
   "metadata": {},
   "source": [
    "#### Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b47408-deae-439f-84ee-ce0e09139d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_connection():\n",
    "\n",
    "    #getting configuration\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "\n",
    "    # Connect to Mysql\n",
    "    conn = mysql.connector.connect(\n",
    "        host = config['mysql']['host'],\n",
    "        user = config['mysql']['user'],\n",
    "        password = config['mysql']['password'],\n",
    "        database = config['mysql']['database']\n",
    "    )\n",
    "        \n",
    "    return conn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce187950-ba61-43da-b3b2-adcc26f74af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(table, field, condition):\n",
    "    \n",
    "    conn = open_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"UPDATE {table} SET {field} WHERE {condition};\")\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74186bbb-ce88-48c8-aaf4-6dd0fbda7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query):\n",
    "    \n",
    "    conn = open_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(query)\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2164244a-d491-42c2-bd19-2f62263ce64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db(query):\n",
    "    conn = open_connection()\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340d245-7251-4b01-879e-187a50119290",
   "metadata": {},
   "source": [
    "#### Specific operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d10c8ed-7d8a-4eae-84b3-2c11f7e62584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_control_db(file_type, file_name = False):\n",
    "    \n",
    "    if file_name == False:\n",
    "        \n",
    "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}'\").to_dict(orient='records')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        check = read_db(f\"SELECT name FROM control WHERE type ='{file_type}' AND name = '{file_name}'\"\n",
    "                       ).to_dict(orient='records')\n",
    "        \n",
    "        if len(check) > 0:\n",
    "            check = True\n",
    "        else:\n",
    "            check = False\n",
    "\n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "808f97b8-16b5-45c4-aa8e-5c2f16c7e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_db(item_name, file_name, download_folder = 'Downloads', type_control = 'download'):\n",
    "    \n",
    "    path = check_folder(download_folder)\n",
    "    \n",
    "    #query to get information from Mysql about control\n",
    "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
    "                             \n",
    "    #check if the file has been unzipped before\n",
    "    if check == False:\n",
    "    \n",
    "        #getting Itens names\n",
    "        item = internetarchive.get_item(item_name)\n",
    "\n",
    "        #downloading\n",
    "        r = item.download(\n",
    "            destdir=path,  # The directory to download files to\n",
    "            ignore_existing=True,  # Skip files that already exist locally\n",
    "            checksum=True,  # Skip files based on checksum\n",
    "            verbose=False,  # Print progress to stdout\n",
    "            retries=100,  # The number of times to retry on failed requests\n",
    "            no_directory=True,  # Download withtout the identifier\n",
    "            files = file_name)\n",
    "        \n",
    "        #getting Metadata\n",
    "        metadata = list(filter(lambda p: p[\"name\"] == file_name, item.item_metadata['files']))\n",
    "\n",
    "        #Adding file name on control list\n",
    "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        sz = metadata[0]['size']\n",
    "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}','{dt}','{type_control}','{sz}')\")\n",
    "    \n",
    "        path_file = path + f'/{file_name}'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        path_file = False   \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff8a6aea-def3-47d2-b9dc-c85e10ba56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_file_db(tar_file, file_name, extration_folder='Extraction', type_control = 'extraction'):\n",
    "    \n",
    "    path = check_folder(extration_folder)\n",
    "    \n",
    "    #query to get information from Mysql about control\n",
    "    check = check_control_db(file_type = type_control, file_name = file_name)\n",
    "    \n",
    "    \n",
    "    #check if the file has been unzipped before\n",
    "    if check == False:\n",
    "\n",
    "        #Extracting file\n",
    "        tar_file.extract(file_name, path=path)\n",
    "\n",
    "        #Adding file name on control list\n",
    "        dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        sz = tar_file.getmember(name=file_name).size\n",
    "        query_db(f\"INSERT INTO control (name, datetime, type, size) VALUES ('{file_name}', '{dt}', '{type_control}', '{sz}')\")\n",
    "        \n",
    "        path_file = path + f'/{file_name}'\n",
    "    \n",
    "    else: #case the file has been unzipped\n",
    "            \n",
    "        path_file = False\n",
    "        \n",
    "    \n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e7af889-caee-42a5-8fd6-af183dbf0dad",
   "metadata": {},
   "source": [
    "read_db(\"SELECT * FROM control WHERE type = 'download'AND name LIKE 'twitter-stream-202208%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee5629-db75-4b84-9840-753eec3876dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checking Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d94a2-200a-4d49-9f8c-0ab5eeb45ee1",
   "metadata": {},
   "source": [
    "## Getting info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45e74d56-fa23-4898-a516-d426a4faf094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = read_db(\"SELECT * FROM control WHERE type = 'extraction'\")\n",
    "down_df = check_control_db(file_type = 'download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec1e704-d36d-437a-9f73-facb9c4a1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'] = df['name'].str[:8].apply(lambda x: datetime.strptime(x, '%Y%m%d').strftime('%Y-%m-%d'))\n",
    "df['data'] = pd.to_datetime(df['data'])\n",
    "df['size'] = df['size'].astype(float)\n",
    "df['count_total'] = df['count_total'].astype(float)\n",
    "df['count_filtered'] = df['count_filtered'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "565029d7-5242-4f98-a47d-d723a1949e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of days: 365 \n",
      "\n",
      "Total Downloaded: 917.96 GiB\n",
      "An average of 2.51 GiB per day downloaded \n",
      "\n",
      "Total of Tweets: 1,412,214,396.0\n",
      "An average of 3,869,081 tweets downloaded per day \n",
      "\n",
      "Tweets filtered: 763,267.0\n",
      "An average of 2,091 tweets filtered per day \n",
      "\n"
     ]
    }
   ],
   "source": [
    "donwloaded = df.groupby(pd.Grouper(key='data', freq='M'))['size'].sum()\n",
    "print(f\"Total of days: {len(down_df)} \\n\")\n",
    "\n",
    "print(f\"Total Downloaded: {humanize(donwloaded.sum())}\")\n",
    "print(f\"An average of {humanize(donwloaded.sum()/len(down_df))} per day downloaded \\n\")\n",
    "\n",
    "print(\"Total of Tweets: {:,}\".format(df['count_total'].sum()))\n",
    "print(f\"An average of {df['count_total'].sum()/len(down_df):,.0f} tweets downloaded per day \\n\")\n",
    "\n",
    "print(\"Tweets filtered: {:,}\".format(df['count_filtered'].sum()))\n",
    "print(f\"An average of {df['count_filtered'].sum()/len(down_df):,.0f} tweets filtered per day \\n\")\n",
    "\n",
    "\n",
    "#deleting df\n",
    "del down_df, donwloaded, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2465e-7322-491a-bc76-2a532015a6f5",
   "metadata": {},
   "source": [
    "## Checking Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fcb9042-08dd-4158-b997-3af1ed4790b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_control = read_db(f\"SELECT name FROM control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4151ad9-6d6b-43db-babc-1524cfae5ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't find duplicated\n"
     ]
    }
   ],
   "source": [
    "duplicated = names_control.duplicated(['name'])\n",
    "df_duplicated = names_control.loc[duplicated, :]\n",
    "if len(df_duplicated)> 0 :\n",
    "    print(f'Find {len(df_duplicated)} duplicated')\n",
    "    print(df_duplicados)\n",
    "else:\n",
    "    print(\"Don't find duplicated\") \n",
    "    \n",
    "    \n",
    "#deleting df\n",
    "del duplicated, df_duplicated, names_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8bff9-2b4f-4a04-af4c-83a1d3aa9ec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing to Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9bf59fce-c6b2-4dfd-8c83-d1c0e199373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysql_to_hdfs(path, mysql_table, hdfs_folder = '/CA4' ,split_by_month=False, date_col='created_at'):\n",
    "\n",
    "    path = check_folder(path)\n",
    "\n",
    "    #Loading data from MySQL database\n",
    "    df = read_db(f\"SELECT * FROM {mysql_table}\")\n",
    "    \n",
    "    if split_by_month == True:\n",
    "        \n",
    "        #Adding month\n",
    "        df[date_col] = pd.to_datetime(df[date_col]).dt.tz_localize('UTC')\n",
    "        df['month'] = df[date_col].dt.month\n",
    "\n",
    "        # Splitting DataFrame by month and saving in parquet files \n",
    "        for month in range(1, 13):\n",
    "            month_df = df[df['month'] == month]\n",
    "            month_df.to_parquet(f\"{path}/{mysql_table}_{month}.parquet\")\n",
    "            print(f\"{mysql_table}_{month}.parquet >>> SAVED AT >>> {path}\")\n",
    "            \n",
    "    else:\n",
    "        df.to_parquet(f\"{path}/{mysql_table}.parquet\")\n",
    "    \n",
    "    # Create a client\n",
    "    client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "    # Copy files to hdfs\n",
    "    client.upload(hdfs_folder, path)\n",
    "    print(f\"{path} >>> UPLOADED AT >>> {hdfs_folder}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b2599-f576-4475-b4f0-598f571b0b3e",
   "metadata": {},
   "source": [
    "## Control Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d5157398-8459-46af-aa0a-6cea18654479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hduser/Desktop/CA 2.2/Data Collect/data/control created.\n",
      "/home/hduser/Desktop/CA 2.2/Data Collect/data/control >>> UPLOADED AT >>> /CA4\n"
     ]
    }
   ],
   "source": [
    "mysql_to_hdfs(path = 'data/control', \n",
    "              mysql_table = 'control',\n",
    "              hdfs_folder = '/CA4',\n",
    "              split_by_month = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9436713-8732-474b-b2cd-ea2316047637",
   "metadata": {},
   "source": [
    "## Tweets Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de79ae25-a293-4130-a355-549532bbf5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_1.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_2.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_3.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_4.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_5.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_6.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_7.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_8.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_9.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_10.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_11.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "tweets_12.parquet >>> SAVED AT >>> /home/hduser/Desktop/CA 2.2/Data Collect/data/tweets\n",
      "/home/hduser/Desktop/CA 2.2/Data Collect/data/tweets >>> UPLOADED AT >>> /CA4\n"
     ]
    }
   ],
   "source": [
    "mysql_to_hdfs(path = 'data/tweets', \n",
    "              mysql_table = 'tweets',\n",
    "              hdfs_folder = '/CA4',\n",
    "              split_by_month = True,\n",
    "              date_col='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a047f13-0ea3-41a4-8356-2adfff2a4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
